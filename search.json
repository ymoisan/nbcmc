[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbcmc",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "nbcmc"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nbcmc",
    "section": "Install",
    "text": "Install\npip install nbcmc",
    "crumbs": [
      "nbcmc"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "nbcmc",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "nbcmc"
    ]
  },
  {
    "objectID": "sqlite.html",
    "href": "sqlite.html",
    "title": "nbcmc",
    "section": "",
    "text": "import sqlite3, getpass\nimport pandas as pd\nimport polars as pl\nfrom itables import show\n# import itables.options as opt\n# opt.maxBytes = 131072\n\n\n\nsqlite_dir = getpass.getpass(prompt='Base directory for SQLite files : ')\n\n\n#connection = sqlite3.connect(f\"{sqlite_dir}synop_b.2024031112.sqlite\")\n\nconnection = sqlite3.connect(f\"{sqlite_dir}2024031318__sqlite\")\ncursor = connection.cursor()\n\nquery = \"SELECT name FROM sqlite_master WHERE type='table';\"\ncursor.execute(query)\ntables = cursor.fetchall() # ('DATA',) ('HEADER',) ('OmF',)\n\nfor table in tables:\n    print(table)\n\nconnection.close()\n\n\n# connection = sqlite3.connect(f\"{sqlite_dir}synop_b.2024031112.sqlite\")\nconnection = sqlite3.connect(f\"{sqlite_dir}2024031318__sqlite\")\n# burp2rdb\n\ndata_table = 'DATA'\nheader_table = 'HEADER'\n# Define the SQL query you want to execute\nquery_data = f\"SELECT * FROM {data_table};\"\nquery_header = f\"SELECT * FROM {header_table};\"\n\n# Execute the query and store the result in a DataFrame\nresult_data = pd.read_sql_query(query_data, connection)\nresult_data_arrow = pd.read_sql_query(query_data, connection, dtype_backend='pyarrow')\nresult_header = pd.read_sql_query(query_header, connection)\n\nconnection.close()\n\n\nresult_data_pl_df = pl.from_pandas(result_data)\n\n\nprint(type(result_data))\nprint(type(result_data_pl_df))\n\n\ndelta_lake_df = pl.scan_delta(\"tar_swob_no_optimization\")\n\n\nprint(result_data.dtypes)\n\n\nprint(result_data_pl_df.dtypes)\n\n\n# result_data_arrow = pd.read_sql_query(query_data, connection, dtype_backend='pyarrow')\nprint(result_data_arrow.dtypes)\n\n\n# result_data_arrow\n# show(result_data_arrow, buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]) # ... (downsampled from 453,375x13 ...\nshow(result_data_pl_df.filter(pl.col(\"VARNO\") == 12004)) # ... (downsampled from 25,065x13 to 856x13 as maxBytes=131072)\n\n\nresult_data_pl_df\n\n\n# result_data_pl_df['VARNO'].unique()\nvarno_list = result_data_pl_df['VARNO'].unique().to_list()\n\n# Generator expression to pad codes and join them with '|'\nvarno_list_grep = '|'.join(f\"{code:06}\" for code in varno_list)\n\n#varno_list_grep = '|'.join(map(str, varno_df))\n\n\nvarno_list_grep\n# to supply to `grep -E` '^(001012|001013|002004|002038|005001|006001|007004|007030|007031|010004|010009|010051|010061|010062|010063|010194|011011|011012|011041|012002|012004|012006|012016|012017|013003|013013|013019|013020|013021|013022|013023|013033|013200|013204|020001|020003|020004|020005|020011|020012|020013|020062|020198|020199|020200|020201|020202|020213|022003|022011|022012|022013|022021|022022|022023|022042|022049)'\n\n\ncode_list_df = pl.read_csv(\"codes_burp.csv\", truncate_ragged_lines=True)\n\n\ncode_list_df\n\n\nresult_ID_OBS = pd.merge(result_data, result_header, on=\"ID_OBS\")\n\n\nresult_ID_OBS\n\n\nunique_time_values = result_ID_OBS[\"TIME\"].unique()\nprint(unique_time_values)",
    "crumbs": [
      "sqlite.html"
    ]
  },
  {
    "objectID": "data-challenge-2024.html",
    "href": "data-challenge-2024.html",
    "title": "2024 Public Service Data Challenge test notebook",
    "section": "",
    "text": "EN Project name : Map air pollution data and other key indicators to improve policymaking AKA “AirTIME – Air trend information for managing exposure”\nFR Nom du projet : Cartographie des données de pollution de l’air et autres indicateurs pour l’aide à l’élaboration des politiques\nTeam members / Équipe : - Rita So Team Lead / Gestionnaire de projet - Zoe Davis Deputy Team Lead / Gestionnaire de projet adjointe - Kumari Gurusami NRCan / RNCan - Ke Gai ECCC - Andrea Hhazzawi TPSGC / PWGSC - Charles Ryan Haynes ECCC - Nicole Johnson AGR - Yves Moisan ECCC -&gt; NRCan / RNCan/ RNCan (01/04/2024)\n\nThe challenge for me :“My understanding is that the datasets (ambient monitoring, emission data, AQ modelling) are not present in one consolidated location online for ECCC access.”\n\nIOW : there are data silos.\nI believe once that problem of data access is out of the way half the problem (of having an understanding of the data to enlighten decision-making) will be solved.\n\nJust to show we can embed equations in a notebook\n\\[\\begin{equation}\ne^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i\n\\end{equation}\\]\nTOdo ; map\n\nimport datetime as dt\nfrom datetime import date\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport polars as pl\nimport polars.selectors as cs\nimport pyarrow as pa\nfrom deltalake import DeltaTable  # S3FileSystem ??\nfrom deltalake.writer import write_deltalake\nfrom great_tables import GT, html, md\nfrom great_tables.data import islands\nfrom tabulate import tabulate\nfrom tqdm.notebook import tqdm\nimport hvplot.pandas # Won't be needed hopefully; we'll be using Polars\nimport hvplot.polars\n\n\nfrom great_tables import GT, md, html, style, loc\nfrom great_tables.data import airquality, islands\n\n\nairquality_mini = airquality.head(10).assign(Year = 1973)\npl_airquality = pl.DataFrame(airquality_mini).select(\n    \"Year\", \"Month\", \"Day\", \"Ozone\", \"Solar_R\", \"Wind\", \"Temp\"\n)\ngt_air = GT(pl_airquality)\n\n(\n    gt_air\n\n    # Table header ----\n    .tab_header(\n        title = \"New York Air Quality Measurements\",\n        subtitle = \"Daily measurements in New York City (May 1-10, 1973)\"\n    )\n\n    # Table column spanners ----\n    .tab_spanner(\n        label = \"Time\",\n        columns = [\"Year\", \"Month\", \"Day\"]\n    )\n    .tab_spanner(\n        label = \"Measurement\",\n        columns = [\"Ozone\", \"Solar_R\", \"Wind\", \"Temp\"]\n    )\n    .cols_label(\n        Ozone = html(\"Ozone,&lt;br&gt;ppbV\"),\n        Solar_R = html(\"Solar R.,&lt;br&gt;cal/m&lt;sup&gt;2&lt;/sup&gt;\"),\n        Wind = html(\"Wind,&lt;br&gt;mph\"),\n        Temp = html(\"Temp,&lt;br&gt;&deg;F\")\n    )\n\n    # Table styles ----\n    .tab_style(\n        style.fill(\"lightyellow\"),\n        loc.body(\n            columns = cs.all(),\n            rows = pl.col(\"Wind\") == pl.col(\"Wind\").max()\n        )\n    )\n    .tab_style(\n        style.text(weight = \"bold\"),\n        loc.body(\"Wind\", pl.col(\"Wind\") == pl.col(\"Wind\").max())\n    )\n)\n\n\ntype(airquality_mini)\ntype(pl_airquality)\n\n\n\nAQHI observation communities\n\nAQHI_communities_df = pl.read_json(\"aqhi_community.geojson\")\n# From url = \"https://dd.weather.gc.ca/air_quality/aqhi/atl/observation/monthly/csv/202302_MONTHLY_AQHI_ATL_SiteObs_BACKFILLED.csv\"\n#AQHI_stations_df = pl.read_json(url)\n\n\nAQHI_communities_df\n\n\n\nAir Pollutant Emission Inventory\nWeb page\n\nEN_APEI_Can_Prov_Terr = pl.scan_csv(\"EN_APEI-Can-Prov_Terr.csv\")\n\n\nAPEI = EN_APEI_Can_Prov_Terr.collect()\nAPEI\n\n\n# Your existing query\n# Define the start and end dates; will be used for the graph title\n#start_date = date(2024, 3, 4)\n#end_date = date(2024, 3, 8)\n\nAPEI_query = (\n    APEI\n    .filter(pl.col(\"Region\") == \"AB\")\n    .sort(pl.col(\"Source\"), descending=False)\n)\n\n\nAPEI_query\n\n\n# Plot the DataFrame using hvplot\nplot = APEI_query.hvplot.line(x='Year', \n                                    y='TPM (t)', \n                                    by='Source', \n                                    title=f\"TPM (t) values for Alberta\")\n\n# Set the y-axis label to include the 'uom' value\n# Assuming 'uom' is a constant value for all rows in your filtered DataFrame\nuom = \"??\" # This should be dynamically fetched if it varies\nplot.opts(ylabel=f\"TPM (t) units = {uom}\")\nplot.opts(xlabel=f\"Year\")\nplot\n\n\n\nAQHI observations\nAtlantic realtime observation web page\n\n# AQHI_ATL_obs = pl.scan_csv(\"2024031007_AQHI_ATL_SiteObs.csv\")\nAQHI_ATL_obs_guessed_date = pl.read_csv(\"2024031007_AQHI_ATL_SiteObs.csv\", try_parse_dates=True) # WOW : that turned \"2024-03-10\" to an actual date object !!\n\n\n# AQHI_ATL_obs\n\n\nAQHI_ATL_obs_guessed_date\n\n\n# Your existing query\n# Define the start and end dates; will be used for the graph title\nstart_date = date(2024, 3, 4)\nend_date = date(2024, 3, 8)\n\nbasic_query_read = (\n    AQHI_ATL_obs_guessed_date\n    .filter(pl.col(\"Date\").is_between(start_date, end_date))\n    .sort(pl.col(\"Date\"), descending=True)\n)\n\n\nbasic_query_read\n\n\n# Plot the DataFrame using hvplot\nplot = basic_query_read.hvplot.line(x='Date', \n                                    y='AADCE', \n                                    by='Hour (UTC)', \n                                    title=f\"AADCE values between {start_date} and {end_date}\")\n\n# Set the y-axis label to include the 'uom' value\n# Assuming 'uom' is a constant value for all rows in your filtered DataFrame\nuom = \"??\" # This should be dynamically fetched if it varies\nplot.opts(ylabel=f\"Temperature ?? ({uom})\")\nplot.opts(xlabel=f\"Date J/MM\")\nplot\n\n\n\nAQHI forecast\nAtlantic forecast web page\n[TXT] 2024031800_AQHI_ATL_UMOSAQMIST.csv    2024-03-18 03:40  9.4K  \n[TXT] 2024031800_PM2.5_ATL_UMOSAQMIST.csv   2024-03-18 03:40   12K  \n[TXT] 2024031800_O3_ATL_UMOSAQMIST.csv      2024-03-18 03:40   14K  \n[TXT] 2024031800_NO2_ATL_UMOSAQMIST.csv     2024-03-18 03:40   12K \n\nAQHI_ATL_forecast_AQHI = pl.scan_csv(\"2024031800_AQHI_ATL_UMOSAQMIST.csv\")\nAQHI_ATL_forecast_PM2_5 = pl.scan_csv(\"2024031800_PM2.5_ATL_UMOSAQMIST.csv\")\nAQHI_ATL_forecast_O3 = pl.scan_csv(\"2024031800_O3_ATL_UMOSAQMIST.csv\")\nAQHI_ATL_forecast_NO2 = pl.scan_csv(\"2024031800_NO2_ATL_UMOSAQMIST.csv\")\n\n\nAQHI_ATL_forecast_AQHI_df = AQHI_ATL_forecast_AQHI.collect()\nAQHI_ATL_forecast_PM2_5_df = AQHI_ATL_forecast_PM2_5.collect()\nAQHI_ATL_forecast_O3_df = AQHI_ATL_forecast_O3.collect()\nAQHI_ATL_forecast_NO2_df = AQHI_ATL_forecast_NO2.collect()\n\n\n# AQHI_ATL_forecast_AQHI_df #  shape: (25, 74); has \"cgndb\" as field name for station location\n\n# All next forecasts have 'stationId' as field name for station location\n\nAQHI_ATL_forecast_PM2_5_df # shape: (31, 74)\n# AQHI_ATL_forecast_O3_df # shape: (31, 74)\n# AQHI_ATL_forecast_NO2_df # shape: (31, 74)\n\n\n# All next forecasts have 'stationId' as field name for station location\n\nAQHI_ATL_forecast_PM2_5_df # shape: (31, 74)\nAQHI_ATL_forecast_O3_df # shape: (31, 74)\nAQHI_ATL_forecast_NO2_df # shape: (31, 74)\n\nindex = pl.col(\"stationId\").alias(\"Station Id\").struct(pl.col(\"date\").alias(\"date\"))\ncombined_df = pl.concat([df1, df2, df3], index)\n\n\ntype(AQHI_ATL_forecast_AQHI.collect().to_pandas())\n#AQHI_ATL_forecast_PM2_5.collect()\n#AQHI_ATL_forecast_O3.collect()\n#AQHI_ATL_forecast_NO2.collect()\n\n\n# AQHI_ATL_forecast_AQHI.collect().hvplot()\nAQHI_pd_df = AQHI_ATL_forecast_AQHI.collect().to_pandas() # hvplot explorer does not support Polars dataframes yet\nAQHI_numerical_df = AQHI_pd_df.select_dtypes(include=['int64', 'float64']) # Select only columns with numeric values\nAQHI_explorer = AQHI_numerical_df.hvplot.explorer()\n# AQHI_pd_df.columns\n#AQHI_explorer = AQHI_pd_df.hvplot.explorer()\n\n\nAQHI_ATL_obs_guessed_date_explorer = AQHI_ATL_obs_guessed_date.to_pandas().hvplot.explorer()\n# AQHI_ATL_obs_guessed_date_explorer\n\n# AQHI_ATL_forecast_AQHI.collect().hvplot()\n# AQHI_pd_df = AQHI_ATL_forecast_AQHI.collect().to_pandas() # hvplot explorer does not support Polars dataframes yet\n# AQHI_numerical_df = AQHI_pd_df.select_dtypes(include=['int64', 'float64']) # Select only columns with numeric values\n# AQHI_explorer = AQHI_numerical_df.hvplot.explorer()\n# AQHI_pd_df.columns\n#AQHI_explorer = AQHI_pd_df.hvplot.explorer()\n\n\nAQHI_ATL_obs_guessed_date_explorer\n\n\nfrom bokeh.sampledata.penguins import data as df\n\ndf.head(2)\ntype(df)\n\n\nhvexplorer = df.hvplot.explorer()\nhvexplorer",
    "crumbs": [
      "2024 Public Service Data Challenge test notebook"
    ]
  },
  {
    "objectID": "solar-potential.html",
    "href": "solar-potential.html",
    "title": "Demo potentiel solaire – Halifax",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport ast\nimport operator\nfrom typing import Callable, Iterator\n\nimport pyarrow.parquet as pq\nimport pyarrow.compute as pc\nimport overturemaps\nfrom palettable.colorbrewer.sequential import Reds_6\nfrom palettable.colorbrewer.diverging import Spectral_5\nfrom pathlib import Path\nimport polars as pl\nfrom deltalake import DeltaTable, write_deltalake\nimport ast\nimport lonboard\nfrom lonboard import Map, PolygonLayer, SolidPolygonLayer, viz\nfrom lonboard.colormap import apply_continuous_cmap\nimport geoarrow.rust.core\nfrom geoarrow.rust.io import read_parquet\nimport bokeh, ipyleaflet\nimport geopandas as gpd\nimport leafmap\nfrom matplotlib.colors import LogNorm\n# import leafmap.deckgl as leafmap\nfrom deltalake import write_deltalake\nlonboard.__version__\n\n'0.10.3'\npl.__version__\n\n'1.17.1'\nfrom palettable.matplotlib import Magma_11\nfrom palettable.matplotlib import Viridis_11\n\n# Define parameters for our custom color scheme\ncenter = 180\nmin_value = 90\nmax_value = 270\nnum_colors = 11\n\n# Create a custom color palette using palettable\ncustom_colors = Magma_11.mpl_colors + Viridis_11.mpl_colors[::-1]\ncustom_colors\n\n[(0.0, 0.0, 0.01568627450980392),\n (0.08235294117647059, 0.054901960784313725, 0.2196078431372549),\n (0.23137254901960785, 0.058823529411764705, 0.4392156862745098),\n (0.39215686274509803, 0.10196078431372549, 0.5019607843137255),\n (0.5490196078431373, 0.1607843137254902, 0.5058823529411764),\n (0.7176470588235294, 0.21568627450980393, 0.4745098039215686),\n (0.8705882352941177, 0.28627450980392155, 0.40784313725490196),\n (0.9647058823529412, 0.43137254901960786, 0.3607843137254902),\n (0.996078431372549, 0.6235294117647059, 0.42745098039215684),\n (0.996078431372549, 0.8117647058823529, 0.5725490196078431),\n (0.9882352941176471, 0.9921568627450981, 0.7490196078431373),\n (0.9921568627450981, 0.9058823529411765, 0.1450980392156863),\n (0.7411764705882353, 0.8745098039215686, 0.14901960784313725),\n (0.47843137254901963, 0.8196078431372549, 0.3176470588235294),\n (0.25882352941176473, 0.7450980392156863, 0.44313725490196076),\n (0.13333333333333333, 0.6588235294117647, 0.5176470588235295),\n (0.12941176470588237, 0.5686274509803921, 0.5490196078431373),\n (0.16470588235294117, 0.47058823529411764, 0.5568627450980392),\n (0.20784313725490197, 0.37254901960784315, 0.5529411764705883),\n (0.2549019607843137, 0.26666666666666666, 0.5294117647058824),\n (0.2823529411764706, 0.1450980392156863, 0.4627450980392157),\n (0.26666666666666666, 0.00392156862745098, 0.32941176470588235)]\n# Colors at https://natural-resources.canada.ca/energy/energy-sources-distribution/renewables/solar-photovoltaic-energy/tools-solar-photovoltaic-energy/photovoltaic-and-solar-resource-maps/18366\n# Annual PV potential; south facing with latitude tilt\ngc_pv_colors = {\n    '0-500': '#010080',\n    '500-600': '#0000CC',\n    '600-700': '#0099FF',\n    '700-800': '#009999',\n    '800-900': '#01CC00',\n    '900-1000': '#00FF01',\n    '1000-1100': '#CCFF00',\n    '1100-1200': '#FFFF00',\n    '1200-1300': '#FFCC00',\n    '1300-1400': '#FE9900',\n    '1400+': '#FE332D'\n}\n# Function to convert hexadecimal to RGBA\ndef hex_to_rgba(hex_color):\n    hex_color = hex_color.lstrip('#')\n    if len(hex_color) == 3:\n        hex_color = ''.join([c*2 for c in hex_color])\n    return [int(hex_color[i:i+2], 16) for i in (0, 2, 4)] + [255]\n\n# Get the list of colors and convert them to RGBA\ncolor_list = [hex_to_rgba(color) for color in gc_pv_colors.values()]\n\n# Convert the list to a numpy array and ensure uint8 type\ncolor_array = np.array(color_list, dtype=np.uint8)\ndef hex_to_rgba(hex_color):\n    hex_color = hex_color.lstrip('#')\n    return [int(hex_color[i:i+2], 16) for i in (0, 2, 4)] + [255]\ncolor_list = [hex_to_rgba(color) for color in gc_pv_colors.values()]\n\nprint(color_list)\n\n[[1, 0, 128, 255], [0, 0, 204, 255], [0, 153, 255, 255], [0, 153, 153, 255], [1, 204, 0, 255], [0, 255, 1, 255], [204, 255, 0, 255], [255, 255, 0, 255], [255, 204, 0, 255], [254, 153, 0, 255], [254, 51, 45, 255]]\ncolor_list = np.array([[1, 8, 128, 255], [0, 0, 204, 255], [0, 153, 255, 255], [0, 153, 153, 255],\n                       [1, 204, 0, 255], [0, 255, 1, 255], [204, 255, 0, 255], [255, 255, 0, 255],\n                       [255, 204, 0, 255], [254, 153, 0, 255], [254, 51, 45, 255]])\nfrom palettable.cubehelix import Cubehelix\npalette = Cubehelix.make(start_hue=1, end_hue=1, n=16)\npalette.mpl_colormap\n\ncustom_cubehelix  underbad over \n\n\n\nMagma_11.mpl_colormap\n\nMagma_11  underbad over \n\n\n\nBlues_8.mpl_colors\n\n\ncustom_colors\n\n\ngeoarrow.rust.core.__version__\n\n\nlonboard.__version__\n\n\nOpen gpkg with roof data\n\nhalifax_hood = Path(\"Halifax_aoi_2D_5.gpkg\") # Autobuilding lidar\n\n\nhalifax_hood_3D = Path(\"Halifax_0_0_1_aoi_3D.gpkg\") # Autobuilding lidar\n\n\nhalifax_hood_LOD = Path(\"Halifax_0_0_1_aoi_CityJSON_LOD_2.2_Demo.gpkg\") # Autobuilding lidar\n\n\n# gdf = gpd.read_file(halifax_hood)\n\n#gdf = gpd.read_file(halifax_hood)\ngdf_3D = gpd.read_file(halifax_hood_3D)\ngdf_LOD = gpd.read_file(halifax_hood_LOD)\n\n\n#gdf = gpd.read_file(halifax_hood, convert_categoricals=False)\n\n\ngdf_3D\n\n\n\n\n\n\n\n\nSHAPE_Area\nSHAPE_Leng\nacqtech\nacqtech_en\nacqtech_fr\nbldgarea\ncomment\ndatemax\ndatemin\nelevmax\nelevmin\nfeature_id\nh_ground\nhaccmax\nhaccmin\nheightmax\nheightmin\nmd_id\nprovider\nprovideren\nproviderfr\nqltylvl\nqltylvl_en\nqltylvl_fr\nroof_type\nvaccmax\nvaccmin\ngeoflow_ID\nbuilding_part_ID\nautobuilding_ID\nlod\nsurface_count\nground_surface_count\nwall_surface_count\nroof_surface_count\nroof_surface_sloped_count\nroof_surface_flat_count\nfootprint_perimeter\nground_Z\nroof_min_Z\nroof_max_Z\nroof_height_range\ntotal_surface_area\ntotal_ground_area\ntotal_wall_area\ntotal_roof_area\nroof_sloped_area\nroof_flat_area\nactual_volume\nsurface_areas\nsurface_azimuths\nsurface_inclinations\nwall_surface_areas\nwall_surface_azimuths\nwall_surface_inclinations\nroof_surface_areas\nroof_surface_azimuths\nroof_surface_inclinations\nroof_surface_types\nsouthern_azimuth\nsouthern_area\nsouthern_rooftype\ngeometry\n\n\n\n\n0\n0.0\n0.0\n1360\nLidar\nLidar\n83.24\nNone\n20180530\n20180509\n51.26\n50.77\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n50.79\n2.0\n2.0\n8.57\n2.03\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n1\n1-0\n10\n2.2\n44\n1\n39\n4\n4\n0\n41.29\n50.79\n52.97\n60.03\n7.06\n478.92\n83.23\n287.74\n99.59\n99.59\n0.00\n618.30\n[85.93, 6.13, 27.13, 0.69, 7.6, 0.43, 0.72, 13...\n[0.0, 35.27, 32.74, 32.74, 218.13, 125.27, 218...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[6.13, 27.13, 0.69, 7.6, 0.43, 0.72, 13.12, 11...\n[35.27, 32.74, 32.74, 218.13, 125.27, 218.13, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[57.54, 0.07, 8.06, 39.58]\n[307.69, 38.48, 219.04, 127.69]\n[33.82, 29.8, 28.43, 33.82]\n['sloped', 'sloped', 'sloped', 'sloped']\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 50.79...\n\n\n1\n0.0\n0.0\n1360\nLidar\nLidar\n170.73\nNone\n20180530\n20180509\n51.02\n49.61\ne758c42a-5725-4cc9-9ee9-60432c998262\n49.66\n2.0\n2.0\n10.08\n1.59\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n10\n10-0\n198\n2.2\n46\n1\n37\n8\n6\n2\n59.26\n49.66\n52.78\n60.59\n7.81\n856.61\n170.77\n474.07\n190.03\n177.47\n12.56\n1563.10\n[192.14, 8.71, 2.9, 9.57, 8.37, 15.31, 0.09, 4...\n[0.0, 333.95, 62.67, 64.46, 153.95, 62.67, 62....\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[8.71, 2.9, 9.57, 8.37, 15.31, 0.09, 4.81, 0.8...\n[333.95, 62.67, 64.46, 153.95, 62.67, 62.67, 1...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[27.49, 8.04, 4.53, 8.03, 24.63, 8.49, 65.35, ...\n[333.43, 90.0, 90.0, 152.97, 152.49, 331.05, 6...\n[32.57, 0.57, 0.57, 33.53, 32.79, 32.29, 22.69...\n['sloped', 'flat', 'flat', 'sloped', 'sloped',...\n152.97\n8.03\nsloped\nMULTIPOLYGON Z (((451937.176 4939701.398 49.66...\n\n\n2\n0.0\n0.0\n1360\nLidar\nLidar\n160.10\nNone\n20180530\n20180509\n49.82\n48.73\nb9293b46-ff0c-4297-beaf-84a3bfff8641\n48.76\n2.0\n2.0\n7.19\n2.01\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n100\n100-0\n2127\n2.2\n77\n1\n69\n7\n6\n1\n56.25\n48.76\n51.98\n56.60\n4.62\n706.10\n157.31\n341.46\n168.08\n147.78\n20.30\n1012.58\n[174.48, 8.23, 5.85, 3.6, 0.25, 1.64, 1.41, 3....\n[0.0, 309.73, 277.48, 127.67, 219.29, 186.34, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[8.23, 5.85, 3.6, 0.25, 1.64, 1.41, 3.85, 4.2,...\n[309.73, 277.48, 127.67, 219.29, 186.34, 71.93...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[3.42, 27.21, 9.26, 66.45, 63.38, 11.17, 9.28]\n[38.93, 180.0, 306.66, 218.66, 38.66, 308.33, ...\n[19.57, 0.0, 32.24, 18.62, 18.62, 33.13, 31.85]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped...\n180.00\n27.21\nflat\nMULTIPOLYGON Z (((451773.27 4939996.155 48.76,...\n\n\n3\n0.0\n0.0\n1360\nLidar\nLidar\n119.93\nDetection of Lidar points classified as ground...\n20180530\n20180509\n46.43\n44.41\nab5aa68b-d4c8-4f6a-ae65-8e34c32a0347\n44.59\n2.0\n2.0\n6.20\n2.01\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n101\n101-0\n2141\n2.2\n33\n1\n29\n3\n2\n1\n45.37\n44.59\n48.46\n52.52\n4.07\n482.99\n98.17\n270.36\n106.22\n76.95\n29.27\n604.33\n[102.34, 4.96, 64.06, 1.76, 7.53, 1.75, 0.93, ...\n[0.0, 119.89, 211.46, 211.46, 211.46, 301.94, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[4.96, 64.06, 1.76, 7.53, 1.75, 0.93, 9.57, 23...\n[119.89, 211.46, 211.46, 211.46, 301.94, 37.67...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[37.98, 33.34, 38.97]\n[214.05, 90.0, 34.05]\n[26.39, 0.57, 26.64]\n['sloped', 'flat', 'sloped']\n214.05\n37.98\nsloped\nMULTIPOLYGON Z (((451637.45 4939898.7 44.59, 4...\n\n\n4\n0.0\n0.0\n1360\nLidar\nLidar\n125.16\nNone\n20180530\n20180509\n64.59\n62.93\n70a3ee39-3ee5-47a6-b757-0cc9cd3674b5\n62.93\n2.0\n2.0\n9.42\n1.53\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n102\n102-0\n2161\n2.2\n60\n1\n53\n6\n5\n1\n50.00\n62.93\n65.19\n73.56\n8.37\n664.59\n125.16\n371.92\n146.42\n132.54\n13.87\n1007.37\n[130.89, 16.79, 44.05, 10.77, 26.08, 1.09, 0.2...\n[0.0, 211.46, 301.46, 302.74, 31.46, 301.46, 2...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[16.79, 44.05, 10.77, 26.08, 1.09, 0.21, 4.64,...\n[211.46, 301.46, 302.74, 31.46, 301.46, 264.29...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[28.39, 35.72, 27.46, 5.61, 44.7, 19.89]\n[301.68, 211.61, 122.23, 33.31, 33.69, 180.0]\n[33.64, 27.23, 32.92, 24.71, 25.68, 0.0]\n['sloped', 'sloped', 'sloped', 'sloped', 'slop...\n180.00\n19.89\nflat\nMULTIPOLYGON Z (((452287.93 4939628.576 62.93,...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303\n0.0\n0.0\n1360\nLidar\nLidar\n166.19\nNone\n20180530\n20180509\n48.94\n46.77\n3213e856-609b-4426-8c1e-223a04f7678c\n46.79\n2.0\n2.0\n9.80\n2.80\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n95\n95-0\n1968\n2.2\n60\n1\n54\n5\n4\n1\n54.93\n46.79\n49.80\n57.58\n7.77\n867.81\n166.20\n471.66\n196.38\n177.69\n18.69\n1493.43\n[175.99, 0.83, 5.64, 4.05, 5.34, 4.29, 9.46, 1...\n[0.0, 132.97, 176.57, 312.97, 132.97, 222.97, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[0.83, 5.64, 4.05, 5.34, 4.29, 9.46, 19.28, 14...\n[132.97, 176.57, 312.97, 132.97, 222.97, 222.9...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[36.37, 4.69, 5.74, 92.79, 80.57]\n[180.0, 135.73, 314.27, 43.96, 223.96]\n[0.0, 33.94, 33.94, 22.92, 22.92]\n['flat', 'sloped', 'sloped', 'sloped', 'sloped']\n180.00\n36.37\nflat\nMULTIPOLYGON Z (((451802.675 4939824.516 46.79...\n\n\n304\n0.0\n0.0\n1360\nLidar\nLidar\n186.30\nNone\n20180530\n20180509\n50.01\n48.25\n746c52b3-98f5-4600-a161-4cc54cfda255\n48.20\n2.0\n2.0\n8.05\n1.65\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n96\n96-0\n1983\n2.2\n35\n1\n29\n5\n4\n1\n57.79\n48.20\n50.40\n56.98\n6.58\n797.35\n186.31\n353.98\n211.31\n200.95\n10.36\n1273.77\n[201.6, 3.88, 1.14, 1.55, 1.07, 49.98, 4.89, 2...\n[0.0, 43.78, 310.54, 43.78, 224.19, 44.19, 314...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[3.88, 1.14, 1.55, 1.07, 49.98, 4.89, 25.81, 1...\n[43.78, 310.54, 43.78, 224.19, 44.19, 314.19, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[19.49, 19.67, 90.73, 101.59, 10.28]\n[42.98, 222.27, 132.27, 312.27, 180.0]\n[36.93, 36.62, 26.62, 26.62, 0.57]\n['sloped', 'sloped', 'sloped', 'sloped', 'flat']\n180.00\n10.28\nflat\nMULTIPOLYGON Z (((451842.467 4939849.468 48.2,...\n\n\n305\n0.0\n0.0\n1360\nLidar\nLidar\n85.72\nDetection of Lidar points classified as ground...\n20180530\n20180509\n50.22\n48.35\n437e2418-92a3-48c9-a988-e5c7ee58ad37\n49.62\n2.0\n2.0\n8.74\n1.28\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n97\n97-0\n2004\n2.2\n55\n1\n48\n6\n5\n1\n43.87\n49.62\n50.94\n58.57\n7.63\n467.15\n85.72\n281.62\n96.84\n82.60\n14.24\n559.44\n[85.74, 0.36, 1.08, 1.4, 1.08, 0.06, 0.09, 0.4...\n[0.0, 254.85, 160.84, 214.01, 305.6, 15.71, 25...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[0.36, 1.08, 1.4, 1.08, 0.06, 0.09, 0.44, 0.22...\n[254.85, 160.84, 214.01, 305.6, 15.71, 252.47,...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[5.01, 14.25, 33.55, 35.77, 4.84, 6.36]\n[41.5, 225.0, 218.66, 38.66, 128.33, 308.5]\n[20.27, 1.62, 22.67, 22.67, 33.44, 34.11]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped...\n218.66\n33.55\nsloped\nMULTIPOLYGON Z (((451697.087 4940060.556 49.62...\n\n\n306\n0.0\n0.0\n1360\nLidar\nLidar\n74.82\nNone\n20180530\n20180509\n54.55\n52.59\n182e58c1-67da-4cfa-af74-19cf4cdf5520\n52.68\n2.0\n2.0\n10.26\n1.25\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n98\n98-0\n2114\n2.2\n37\n1\n31\n5\n4\n1\n39.53\n52.68\n55.66\n63.63\n7.97\n496.63\n72.32\n343.98\n80.27\n72.78\n7.49\n657.93\n[72.32, 5.0, 8.59, 2.23, 0.24, 4.87, 20.81, 5....\n[0.0, 44.59, 128.48, 303.53, 58.06, 213.53, 30...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[5.0, 8.59, 2.23, 0.24, 4.87, 20.81, 5.05, 4.1...\n[44.59, 128.48, 303.53, 58.06, 213.53, 303.53,...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[23.37, 7.49, 8.04, 8.58, 32.86]\n[306.57, 315.0, 35.96, 216.5, 125.36]\n[22.76, 0.81, 38.93, 38.57, 22.45]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped']\n216.50\n8.58\nsloped\nMULTIPOLYGON Z (((452061.691 4939653.73 52.68,...\n\n\n307\n0.0\n0.0\n1360\nLidar\nLidar\n95.90\nNone\n20180530\n20180509\n48.77\n48.35\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n48.40\n2.0\n2.0\n6.21\n1.65\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n99\n99-0\n2121\n2.2\n54\n1\n49\n4\n3\n1\n49.58\n48.40\n50.22\n56.48\n6.26\n463.31\n95.89\n250.01\n104.66\n84.18\n20.48\n480.60\n[95.9, 11.7, 0.81, 0.06, 0.29, 0.26, 1.73, 3.5...\n[0.0, 293.5, 126.07, 81.38, 199.89, 108.07, 17...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[11.7, 0.81, 0.06, 0.29, 0.26, 1.73, 3.54, 0.6...\n[293.5, 126.07, 81.38, 199.89, 108.07, 17.53, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[4.1, 6.85, 85.96, 20.49]\n[305.54, 18.43, 199.23, 180.0]\n[9.96, 24.31, 27.1, 1.72]\n['sloped', 'sloped', 'sloped', 'flat']\n180.00\n20.49\nflat\nMULTIPOLYGON Z (((451634.297 4940024.668 48.4,...\n\n\n\n\n308 rows × 63 columns\n\n\n\n\n\nManually adjust heights to the geoid @ Halifax\n\n# calc for Halifax : 20.447m\n# https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/tools-outils/gpsh.php\n\n# altitude according to PVGIS : 56 m\n\noffset_approx = 22.35  \noffset_website = 20.447  # The amount to subtract from all altitudes\noffset_PVGIS = 56  # The amount to subtract from all altitudes\n\noffset = offset_PVGIS # The amount to subtract from all altitudes\n\n# Apply the translation to all geometries\ngdf_LOD['geometry'] = gdf_LOD['geometry'].translate(xoff=0, yoff=0, zoff=-offset)\n\n\ngdf = gdf_LOD\n\n\npd.set_option('display.max_columns', None)\n#print(gdf.dtypes)\n\n\ngdf.head(50)\n\n\n# specify the columns you want to keep\ncolumns_to_keep = ['feature_id', 'bldgarea', \n                    'roof_surface_areas', 'roof_surface_azimuths', \n                    'roof_surface_inclinations', 'roof_surface_types', 'southern_azimuth', 'southern_area', 'southern_rooftype','geometry']\nlist1 = columns_to_keep\n\ncolumns_to_keep = ['attribute.feature_id', 'attribute.bldgarea', \n                   'attribute.southern_azimuth', 'attribute.southern_area', 'attribute.southern_rooftype', 'attribute.heightmax', 'geometry']\n\nlist2 = columns_to_keep\n\n\ndict1 = {item: item for item in list1}\n\n\ndict1\n\n\ndict2 = {}\nfor key in list1:\n    if f'attribute.{key}' in list2:\n        dict2[key] = f'attribute.{key}'\n    elif key in list2:  # For 'geometry' which doesn't have 'attribute.' prefix\n        dict2[key] = key\n    else:\n        dict2[key] = None\n\n\ndict2\n\n\n# drop columns that are not specified in the list\ngdf = gdf.drop(gdf.columns.drop(columns_to_keep), axis=1, errors='ignore')\n\n\ngdf.head()\n\n\n\n\n\n\n\n\nattribute.bldgarea\nattribute.feature_id\nattribute.heightmax\nattribute.southern_azimuth\nattribute.southern_area\nattribute.southern_rooftype\ngeometry\nattribute.southern_azimuth_transformed\nsolar_potential_index\nnumber_panels\n\n\n\n\n0\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 -5.21...\n141\n3\n4\n\n\n1\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451617.178 4940177.072 1.36,...\n141\n3\n4\n\n\n2\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.472 4940180.855 3.766...\n141\n3\n4\n\n\n3\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.737 4940180.686 -3.02...\n141\n3\n4\n\n\n4\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451607.514 4940169.24 -1.377...\n141\n3\n4\n\n\n\n\n\n\n\n\ndef transform_series_old(series):\n    \"\"\"\n    Transform values in a pandas Series from range [0, 360] to [0, 180].\n    With a maximum at 180, so \n    170 =&gt; 170\n    190 =&gt; 170\n    \n    Args:\n        series (pandas Series): A pandas Series of floating-point values\n    \n    Returns:\n        pandas Series: Transformed Series\n    \"\"\"\n    return series.where(series &lt;= 180, 360 - series)\n\ndef transform_series(series):\n    \"\"\"\n    Transform values in a pandas Series from range [0, 360] to [0, 180].\n    With a maximum at 180, so \n    170 =&gt; 170\n    190 =&gt; 170\n    \n    Args:\n        series (pandas Series): A pandas Series of floating-point values\n    \n    Returns:\n        pandas Series: Transformed Series\n    \"\"\"\n    # Convert series to float if it's not already\n    series = pd.to_numeric(series, errors='coerce')\n    # Round to nearest integer\n    series = series.round().astype(int)\n    \n    # Apply the transformation\n    return series.where(series &lt;= 180, 360 - series)\n\n\n# Function to calculate solar index\n\ndef calculate_solar_potential_old(df, minimum_area=1.65):\n    \"\"\"\n    Calculate the solar potential index and the number of panels for a given roof surface.\n\n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the roof surface data.\n    minimum_area (float, optional): The minimum area required for a roof surface to be considered for solar panels. Default is 1.65 square meters.\n\n    Returns:\n    None\n\n    \"\"\"\n      # Transform to azimuth max = 180 \n    df['attribute.southern_azimuth_transformed'] = df['attribute.southern_azimuth'].apply(lambda x: transform_series(pd.Series([x]))[0])\n    # Calculate the solar potential index\n\n    df['solar_potential_index'] = np.where(\n                                      (df['attribute.southern_rooftype'] == 'flat'), 5,\n                                  np.where(\n                                      (df['attribute.southern_area'] &lt; minimum_area) | (df['attribute.southern_azimuth_transformed'] &lt;= 90), 0,\n                                        np.round((df['attribute.southern_azimuth_transformed'] - 90) / 18).clip(0, 5)\n                                    )\n                                  ).astype(int)\n\n    df['number_panels'] = np.floor(df['attribute.southern_area'] / minimum_area).astype(int)\n\ndef calculate_solar_potential(df, minimum_area=1.65):\n    \"\"\"\n    Calculate the solar potential index and the number of panels for a given roof surface.\n\n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the roof surface data.\n    minimum_area (float, optional): The minimum area required for a roof surface to be considered for solar panels. Default is 1.65 square meters.\n\n    Returns:\n    None\n    \"\"\"\n    # Convert area to float\n    df['attribute.southern_area'] = pd.to_numeric(df['attribute.southern_area'], errors='coerce')\n\n    # Transform to azimuth max = 180 and convert to float\n    df['attribute.southern_azimuth_transformed'] = transform_series(df['attribute.southern_azimuth'])\n\n    # Calculate the solar potential index\n    df['solar_potential_index'] = np.where(\n        (df['attribute.southern_rooftype'] == 'flat'), 5,\n        np.where(\n            (df['attribute.southern_area'] &lt; minimum_area) | (df['attribute.southern_azimuth_transformed'] &lt;= 90), 0,\n            np.round((df['attribute.southern_azimuth_transformed'] - 90) / 18).clip(0, 5)\n        )\n    ).astype(int)\n\n    df['number_panels'] = np.floor(df['attribute.southern_area'] / minimum_area).astype(int)\n\n\ndef calculate_solar_azimuth_index(df, index_max=10):\n    \"\"\"\n    Calculate the solar azimuth index based on the southern azimuth.\n    \n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the 'attribute.southern_azimuth' column.\n    index_max (int): The maximum value for the solar azimuth index. Default is 10.\n    \n    Returns:\n    None (modifies the DataFrame in place)\n    \"\"\"\n    \n    # Define the step size\n    step = 90 / index_max\n    \n    # Create the conditions and values for the index\n    conditions = [\n        (df['attribute.southern_azimuth'] &lt; 90) | (df['attribute.southern_azimuth'] &gt; 270),\n        (df['attribute.southern_azimuth'] &gt;= 180 - step/2) & (df['attribute.southern_azimuth'] &lt;= 180 + step/2)\n    ]\n    values = [0, index_max]\n    \n    # Add conditions and values for decreasing index on both sides of 180\n    for i in range(1, index_max):\n        lower_bound = 180 - (i+0.5)*step\n        upper_bound = 180 + (i+0.5)*step\n        conditions.append((df['attribute.southern_azimuth'] &gt;= lower_bound) & (df['attribute.southern_azimuth'] &lt; 180 - (i-0.5)*step))\n        conditions.append((df['attribute.southern_azimuth'] &gt; 180 + (i-0.5)*step) & (df['attribute.southern_azimuth'] &lt;= upper_bound))\n        values.extend([index_max - i, index_max - i])\n    \n    # Apply the conditions to create the solar_azimuth_index\n    df['solar_azimuth_index'] = np.select(conditions, values, default=0)\n\n# Usage\n# calculate_solar_azimuth_index(df, index_max=10)\n\n\ncalculate_solar_potential(gdf) # modifies gdf\n#calculate_solar_azimuth_index(gdf, index_max=10)\n\n\n\nArrow table stuff\n\ngdf_arrow = gdf.to_arrow()\n\n\ntype(gdf_arrow)\n\n\ngdf_arrow # The returned data object needs to be consumed by a library implementing the Arrow PyCapsule Protocol. For example, wrapping the data as a pyarrow.Table\n\n\ntable = pa.table(gdf_arrow)\n\n\ntable.column_names\n\n\ndef apply_operation(table, col1, col2, op):\n    # Convert ChunkedArrays to Python lists\n    list1 = table.column(col1).to_pylist()\n    list2 = table.column(col2).to_pylist()\n    \n    # Convert string representations to actual lists of floats\n    list1 = [ast.literal_eval(x) for x in list1]\n    list2 = [ast.literal_eval(x) for x in list2]\n    \n    # Apply the operation element-wise\n    result = []\n    for a, b in zip(list1, list2):\n        result.append([op(x, y) for x, y in zip(a, b)])\n    \n    # Convert the result back to a ChunkedArray\n    result_strings = [str(x) for x in result]\n    return pa.chunked_array([pa.array(result_strings)])\n\n\ndef string_to_float_list(s: str) -&gt; list:\n    return [float(x) for x in ast.literal_eval(s)]\n\ndef apply_operation_gen(col1: pa.ChunkedArray, col2: pa.ChunkedArray, op: Callable) -&gt; Iterator[str]:\n    for item1, item2 in zip(col1, col2):\n        list1 = string_to_float_list(item1.as_py())\n        list2 = string_to_float_list(item2.as_py())\n        result = [op(x, y) for x, y in zip(list1, list2)]\n        yield str(result)\n\ndef apply_operation_v2(table: pa.Table, col1_name: str, col2_name: str, op: Callable) -&gt; pa.ChunkedArray:\n    col1 = table.column(col1_name)\n    col2 = table.column(col2_name)\n    result_list = list(apply_operation_gen(col1, col2, op))\n    return pa.chunked_array([pa.array(result_list)])\n\n\nresult = apply_operation(table, 'roof_surface_azimuths', 'roof_surface_areas', operator.sub)\n\n\nresult_gen = apply_operation_v2(table, 'roof_surface_azimuths', 'roof_surface_areas', operator.sub)\n\n\nresult\n\n\ntable.column('roof_surface_azimuths')[0]\n\n\n\nGDF stuff\n\ngdf\n\n\n\n\n\n\n\n\nattribute.bldgarea\nattribute.feature_id\nattribute.heightmax\nattribute.southern_azimuth\nattribute.southern_area\nattribute.southern_rooftype\ngeometry\nattribute.southern_azimuth_transformed\nsolar_potential_index\nnumber_panels\n\n\n\n\n0\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 -5.21...\n141\n3\n4\n\n\n1\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451617.178 4940177.072 1.36,...\n141\n3\n4\n\n\n2\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.472 4940180.855 3.766...\n141\n3\n4\n\n\n3\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.737 4940180.686 -3.02...\n141\n3\n4\n\n\n4\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451607.514 4940169.24 -1.377...\n141\n3\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15809\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451637.207 4940027.548 -1.02...\n180\n5\n12\n\n\n15810\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451631.96 4940022.268 -5.274...\n180\n5\n12\n\n\n15811\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451638.405 4940030.285 -2.74...\n180\n5\n12\n\n\n15812\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451636.792 4940026.709 -1.50...\n180\n5\n12\n\n\n15813\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451639.106 4940023.765 -5.75...\n180\n5\n12\n\n\n\n\n15814 rows × 10 columns\n\n\n\n\naz_index = gdf['attribute.southern_azimuth'].to_numpy()\naz_normalized = gdf['attribute.southern_azimuth_transformed'].to_numpy()\n\n\nSpectral_5.mpl_colormap\n#type(Reds_6)\n# RdBu_9.mpl_colormap\n\nSpectral  underbad over \n\n\n\ncolors = apply_continuous_cmap(normalized_heights, Spectral_5)\n\n\nheights = gdf['attribute.heightmax'].to_numpy()\nheights = np.nan_to_num(heights, nan=1)\n\n\nnormalizer = LogNorm(1, heights.max(), clip=True)\nnormalized_heights = normalizer(heights)\n# normalizer = LogNorm(1, roof_directions.max(), clip=True)\n# normalized_heights = normalizer(roof_directions)\n\n\n# colors = apply_continuous_cmap(az_normalized / 180, RdBu_9) # Reds_6)\ncolors = apply_continuous_cmap(az_index / 10, Reds_6) # Reds_6)\n\n\nprint(gdf['solar_azimuth_index'].value_counts())\n\n\nlayer25 = PolygonLayer.from_geopandas(\n    gdf,\n#    wireframe=True,\n#    extruded=True,\n#    filled=True,\n#    get_fill_color=colors,\n#    get_elevation=heights,\n#    auto_highlight=True,\n #   pickable=True,\n    #get_fill_color=np.flip(colors),\n)\n\n\nlayer3D = PolygonLayer.from_geopandas(\n    gdf_3D,\n#    wireframe=True,\n#    extruded=True,\n#    filled=True,\n#    get_fill_color=colors,\n#    get_elevation=heights,\n#    auto_highlight=True,\n #   pickable=True,\n    #get_fill_color=np.flip(colors),\n)\n\n\nflipped_colors = np.flip(custom_colors)\n\n\n#layerLOD =SolidPolygonLayer.from_geopandas(\nlayerLOD =PolygonLayer.from_geopandas(\n    gdf_LOD,\n    line_joint_rounded=True,\n#    elevation_scale = 0,\n#   wireframe=True,\n#    extruded=True,\n#    filled=True,\n#get_fill_color=Spectral_5,\n#    get_elevation=heights,\n#    auto_highlight=True,\n#   pickable=False,\n    #get_fill_color=np.flip(colors),\n)\n\n\nview_state_halifax = {\n    \"longitude\": -63.55, # Longitude at the map center\n    \"latitude\": 44.62, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 0, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 0, # Bearing angle in degrees. `0` is north.\n}\n\n\nlayer = PolygonLayer(\n    # Select only a few attribute columns from the table\n    # AttributeError: 'geoarrow.rust.core._rust.GeoTable' object [table_parquet_geoarrow_rust] has no attribute 'select'\n    table=table_pq.select([\"feature_id\", \"md_id\", \n                           \"acqtech\", \"acqtech_en\", \"acqtech_fr\",\n                           \"elevmin\", \"elevmax\", \"bldgarea\", \"comment\", \"geometry\"]),\n    #extruded=True,\n    wireframe=False,\n    extruded=False,\n    filled=False,\n    # stroked=False,\n    #get_elevation=heights,\n    #get_fill_color=colors,\n)\n\n\nm = Map(layerLOD,  _height=800)\n#m = Map(layer, view_state=view_state_halifax, _height=800)\n\n\nm\n\n\n\n\n\n\nPhotovoltaic potential of buildings\nAs a function of azimuth angles of rooftop surfaces\n\nm\n\nLet’s color the buildings by their roof_direction”. First, convert the “roof_direction” column to a numpy array, then replace any null values with 1.\n\n# Buggy : TraitError: accessor must have same length as table\nroof_directions = table[\"roof_direction\"].to_numpy()\nroof_directions = np.nan_to_num(roof_directions, nan=1)\n\nThen we can apply a colormap to these heights. For this case, we’ll use the Oranges_9 colormap.\n\nnew_color = Reds_6.hex_colors\n\nUsing apply_continuous_cmap, we can apply our values onto this colormap.\n\ncolors = apply_continuous_cmap(normalized_heights, Oranges_9)\n\n\ncolors\n\n\n# Colors at https://natural-resources.canada.ca/energy/energy-sources-distribution/renewables/solar-photovoltaic-energy/tools-solar-photovoltaic-energy/photovoltaic-and-solar-resource-maps/18366\n# Annual PV potential; south facing with latitude tilt\ngc_pv_colors = {\n    '0-500': '#010080',\n    '500-600': '#0000CC',\n    '600-700': '#0099FF',\n    '700-800': '#009999',\n    '800-900': '#01CC00',\n    '900-1000': '#00FF01',\n    '1000-1100': '#CCFF00',\n    '1100-1200': '#FFFF00',\n    '1200-1300': '#FFCC00',\n    '1300-1400': '#FE9900',\n    '1400+': '#FE332D'\n}\nlen(gc_pv_colors)\n\n11\n\n\n\nimport IPython.display as ipd\n\nfor interval, color in gc_pv_colors.items():\n    ipd.display(ipd.HTML(f\"&lt;span style='background-color:{color}; color:white; padding:5px'&gt;{interval}&lt;/span&gt;\"))\n\n0-500\n\n\n500-600\n\n\n600-700\n\n\n700-800\n\n\n800-900\n\n\n900-1000\n\n\n1000-1100\n\n\n1100-1200\n\n\n1200-1300\n\n\n1300-1400\n\n\n1400+\n\n\nWe create a PolygonLayer with our data, passing in the heights and colors from before.\n\nlayer = PolygonLayer(\n    # Select only a few attribute columns from the table\n    table=table.select([\"attribute.feature_id\",\"geometry\",]),\n    extruded=False,\n    wireframe=False,\n    # extruded=False,\n    # filled=False,\n    # stroked=False,\n    #get_elevation=heights,\n    #get_fill_color=colors,\n)\n\nWe manually set the view state here for the original NYC data so that the map will start pitched. Remove or change this view_state call if you change the input dataset.\n\nview_state_halifax = {\n    \"longitude\": -63.56, # Longitude at the map center\n    \"latitude\": 44.625, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 59.80465353190481, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 13.243243243243244, # Bearing angle in degrees. `0` is north.\n}\n\n\nview_state_nyc = {\n    \"longitude\": -73.98416810282863, # Longitude at the map center\n    \"latitude\": 40.72651721370669, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 59.80465353190481, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 13.243243243243244, # Bearing angle in degrees. `0` is north.\n}\n\n\nview_state_tokyo = {\n    \"longitude\": 139.79, # Longitude at the map center\n    \"latitude\": 35.73, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 600, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 0, # Bearing angle in degrees. `0` is north.\n}\n\n\nview_state_nb = {\n    \"longitude\": -66.646332, # Longitude at the map center, Fredericton\n    \"latitude\": 45.964993, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 600, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 0, # Bearing angle in degrees. `0` is north.\n}\n\n\nview_state_cgy = { \n    \"longitude\": -114, # Longitude at the map center, Fredericton\n    \"latitude\": 51, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 600, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 0, # Bearing angle in degrees. `0` is north.\n}\n\n\n# view_state = view_state_nyc\nview_state = view_state_halifax\n# view_state = view_state_tokyo\n# view_state = view_state_cgy\n#view_state = view_state_nb\n\n\nm = Map(layer, view_state=view_state, _height=800)\n\n\nm",
    "crumbs": [
      "Demo potentiel solaire -- Halifax"
    ]
  },
  {
    "objectID": "solar-potential.html#open-gpkg-with-roof-data",
    "href": "solar-potential.html#open-gpkg-with-roof-data",
    "title": "Demo potentiel solaire – Halifax",
    "section": "Open gpkg with roof data",
    "text": "Open gpkg with roof data\n\nhalifax_hood = Path(\"Halifax_aoi_2D_5.gpkg\") # Autobuilding lidar\n\n\nhalifax_hood_3D = Path(\"Halifax_0_0_1_aoi_3D.gpkg\") # Autobuilding lidar\n\n\nhalifax_hood_LOD = Path(\"Halifax_0_0_1_aoi_CityJSON_LOD_2.2_Demo.gpkg\") # Autobuilding lidar\n\n\n# gdf = gpd.read_file(halifax_hood)\n\n#gdf = gpd.read_file(halifax_hood)\ngdf_3D = gpd.read_file(halifax_hood_3D)\ngdf_LOD = gpd.read_file(halifax_hood_LOD)\n\n\n#gdf = gpd.read_file(halifax_hood, convert_categoricals=False)\n\n\ngdf_3D\n\n\n\n\n\n\n\n\nSHAPE_Area\nSHAPE_Leng\nacqtech\nacqtech_en\nacqtech_fr\nbldgarea\ncomment\ndatemax\ndatemin\nelevmax\nelevmin\nfeature_id\nh_ground\nhaccmax\nhaccmin\nheightmax\nheightmin\nmd_id\nprovider\nprovideren\nproviderfr\nqltylvl\nqltylvl_en\nqltylvl_fr\nroof_type\nvaccmax\nvaccmin\ngeoflow_ID\nbuilding_part_ID\nautobuilding_ID\nlod\nsurface_count\nground_surface_count\nwall_surface_count\nroof_surface_count\nroof_surface_sloped_count\nroof_surface_flat_count\nfootprint_perimeter\nground_Z\nroof_min_Z\nroof_max_Z\nroof_height_range\ntotal_surface_area\ntotal_ground_area\ntotal_wall_area\ntotal_roof_area\nroof_sloped_area\nroof_flat_area\nactual_volume\nsurface_areas\nsurface_azimuths\nsurface_inclinations\nwall_surface_areas\nwall_surface_azimuths\nwall_surface_inclinations\nroof_surface_areas\nroof_surface_azimuths\nroof_surface_inclinations\nroof_surface_types\nsouthern_azimuth\nsouthern_area\nsouthern_rooftype\ngeometry\n\n\n\n\n0\n0.0\n0.0\n1360\nLidar\nLidar\n83.24\nNone\n20180530\n20180509\n51.26\n50.77\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n50.79\n2.0\n2.0\n8.57\n2.03\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n1\n1-0\n10\n2.2\n44\n1\n39\n4\n4\n0\n41.29\n50.79\n52.97\n60.03\n7.06\n478.92\n83.23\n287.74\n99.59\n99.59\n0.00\n618.30\n[85.93, 6.13, 27.13, 0.69, 7.6, 0.43, 0.72, 13...\n[0.0, 35.27, 32.74, 32.74, 218.13, 125.27, 218...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[6.13, 27.13, 0.69, 7.6, 0.43, 0.72, 13.12, 11...\n[35.27, 32.74, 32.74, 218.13, 125.27, 218.13, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[57.54, 0.07, 8.06, 39.58]\n[307.69, 38.48, 219.04, 127.69]\n[33.82, 29.8, 28.43, 33.82]\n['sloped', 'sloped', 'sloped', 'sloped']\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 50.79...\n\n\n1\n0.0\n0.0\n1360\nLidar\nLidar\n170.73\nNone\n20180530\n20180509\n51.02\n49.61\ne758c42a-5725-4cc9-9ee9-60432c998262\n49.66\n2.0\n2.0\n10.08\n1.59\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n10\n10-0\n198\n2.2\n46\n1\n37\n8\n6\n2\n59.26\n49.66\n52.78\n60.59\n7.81\n856.61\n170.77\n474.07\n190.03\n177.47\n12.56\n1563.10\n[192.14, 8.71, 2.9, 9.57, 8.37, 15.31, 0.09, 4...\n[0.0, 333.95, 62.67, 64.46, 153.95, 62.67, 62....\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[8.71, 2.9, 9.57, 8.37, 15.31, 0.09, 4.81, 0.8...\n[333.95, 62.67, 64.46, 153.95, 62.67, 62.67, 1...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[27.49, 8.04, 4.53, 8.03, 24.63, 8.49, 65.35, ...\n[333.43, 90.0, 90.0, 152.97, 152.49, 331.05, 6...\n[32.57, 0.57, 0.57, 33.53, 32.79, 32.29, 22.69...\n['sloped', 'flat', 'flat', 'sloped', 'sloped',...\n152.97\n8.03\nsloped\nMULTIPOLYGON Z (((451937.176 4939701.398 49.66...\n\n\n2\n0.0\n0.0\n1360\nLidar\nLidar\n160.10\nNone\n20180530\n20180509\n49.82\n48.73\nb9293b46-ff0c-4297-beaf-84a3bfff8641\n48.76\n2.0\n2.0\n7.19\n2.01\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n100\n100-0\n2127\n2.2\n77\n1\n69\n7\n6\n1\n56.25\n48.76\n51.98\n56.60\n4.62\n706.10\n157.31\n341.46\n168.08\n147.78\n20.30\n1012.58\n[174.48, 8.23, 5.85, 3.6, 0.25, 1.64, 1.41, 3....\n[0.0, 309.73, 277.48, 127.67, 219.29, 186.34, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[8.23, 5.85, 3.6, 0.25, 1.64, 1.41, 3.85, 4.2,...\n[309.73, 277.48, 127.67, 219.29, 186.34, 71.93...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[3.42, 27.21, 9.26, 66.45, 63.38, 11.17, 9.28]\n[38.93, 180.0, 306.66, 218.66, 38.66, 308.33, ...\n[19.57, 0.0, 32.24, 18.62, 18.62, 33.13, 31.85]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped...\n180.00\n27.21\nflat\nMULTIPOLYGON Z (((451773.27 4939996.155 48.76,...\n\n\n3\n0.0\n0.0\n1360\nLidar\nLidar\n119.93\nDetection of Lidar points classified as ground...\n20180530\n20180509\n46.43\n44.41\nab5aa68b-d4c8-4f6a-ae65-8e34c32a0347\n44.59\n2.0\n2.0\n6.20\n2.01\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n101\n101-0\n2141\n2.2\n33\n1\n29\n3\n2\n1\n45.37\n44.59\n48.46\n52.52\n4.07\n482.99\n98.17\n270.36\n106.22\n76.95\n29.27\n604.33\n[102.34, 4.96, 64.06, 1.76, 7.53, 1.75, 0.93, ...\n[0.0, 119.89, 211.46, 211.46, 211.46, 301.94, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[4.96, 64.06, 1.76, 7.53, 1.75, 0.93, 9.57, 23...\n[119.89, 211.46, 211.46, 211.46, 301.94, 37.67...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[37.98, 33.34, 38.97]\n[214.05, 90.0, 34.05]\n[26.39, 0.57, 26.64]\n['sloped', 'flat', 'sloped']\n214.05\n37.98\nsloped\nMULTIPOLYGON Z (((451637.45 4939898.7 44.59, 4...\n\n\n4\n0.0\n0.0\n1360\nLidar\nLidar\n125.16\nNone\n20180530\n20180509\n64.59\n62.93\n70a3ee39-3ee5-47a6-b757-0cc9cd3674b5\n62.93\n2.0\n2.0\n9.42\n1.53\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n102\n102-0\n2161\n2.2\n60\n1\n53\n6\n5\n1\n50.00\n62.93\n65.19\n73.56\n8.37\n664.59\n125.16\n371.92\n146.42\n132.54\n13.87\n1007.37\n[130.89, 16.79, 44.05, 10.77, 26.08, 1.09, 0.2...\n[0.0, 211.46, 301.46, 302.74, 31.46, 301.46, 2...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[16.79, 44.05, 10.77, 26.08, 1.09, 0.21, 4.64,...\n[211.46, 301.46, 302.74, 31.46, 301.46, 264.29...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[28.39, 35.72, 27.46, 5.61, 44.7, 19.89]\n[301.68, 211.61, 122.23, 33.31, 33.69, 180.0]\n[33.64, 27.23, 32.92, 24.71, 25.68, 0.0]\n['sloped', 'sloped', 'sloped', 'sloped', 'slop...\n180.00\n19.89\nflat\nMULTIPOLYGON Z (((452287.93 4939628.576 62.93,...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303\n0.0\n0.0\n1360\nLidar\nLidar\n166.19\nNone\n20180530\n20180509\n48.94\n46.77\n3213e856-609b-4426-8c1e-223a04f7678c\n46.79\n2.0\n2.0\n9.80\n2.80\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n95\n95-0\n1968\n2.2\n60\n1\n54\n5\n4\n1\n54.93\n46.79\n49.80\n57.58\n7.77\n867.81\n166.20\n471.66\n196.38\n177.69\n18.69\n1493.43\n[175.99, 0.83, 5.64, 4.05, 5.34, 4.29, 9.46, 1...\n[0.0, 132.97, 176.57, 312.97, 132.97, 222.97, ...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[0.83, 5.64, 4.05, 5.34, 4.29, 9.46, 19.28, 14...\n[132.97, 176.57, 312.97, 132.97, 222.97, 222.9...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[36.37, 4.69, 5.74, 92.79, 80.57]\n[180.0, 135.73, 314.27, 43.96, 223.96]\n[0.0, 33.94, 33.94, 22.92, 22.92]\n['flat', 'sloped', 'sloped', 'sloped', 'sloped']\n180.00\n36.37\nflat\nMULTIPOLYGON Z (((451802.675 4939824.516 46.79...\n\n\n304\n0.0\n0.0\n1360\nLidar\nLidar\n186.30\nNone\n20180530\n20180509\n50.01\n48.25\n746c52b3-98f5-4600-a161-4cc54cfda255\n48.20\n2.0\n2.0\n8.05\n1.65\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n96\n96-0\n1983\n2.2\n35\n1\n29\n5\n4\n1\n57.79\n48.20\n50.40\n56.98\n6.58\n797.35\n186.31\n353.98\n211.31\n200.95\n10.36\n1273.77\n[201.6, 3.88, 1.14, 1.55, 1.07, 49.98, 4.89, 2...\n[0.0, 43.78, 310.54, 43.78, 224.19, 44.19, 314...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[3.88, 1.14, 1.55, 1.07, 49.98, 4.89, 25.81, 1...\n[43.78, 310.54, 43.78, 224.19, 44.19, 314.19, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[19.49, 19.67, 90.73, 101.59, 10.28]\n[42.98, 222.27, 132.27, 312.27, 180.0]\n[36.93, 36.62, 26.62, 26.62, 0.57]\n['sloped', 'sloped', 'sloped', 'sloped', 'flat']\n180.00\n10.28\nflat\nMULTIPOLYGON Z (((451842.467 4939849.468 48.2,...\n\n\n305\n0.0\n0.0\n1360\nLidar\nLidar\n85.72\nDetection of Lidar points classified as ground...\n20180530\n20180509\n50.22\n48.35\n437e2418-92a3-48c9-a988-e5c7ee58ad37\n49.62\n2.0\n2.0\n8.74\n1.28\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n97\n97-0\n2004\n2.2\n55\n1\n48\n6\n5\n1\n43.87\n49.62\n50.94\n58.57\n7.63\n467.15\n85.72\n281.62\n96.84\n82.60\n14.24\n559.44\n[85.74, 0.36, 1.08, 1.4, 1.08, 0.06, 0.09, 0.4...\n[0.0, 254.85, 160.84, 214.01, 305.6, 15.71, 25...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[0.36, 1.08, 1.4, 1.08, 0.06, 0.09, 0.44, 0.22...\n[254.85, 160.84, 214.01, 305.6, 15.71, 252.47,...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[5.01, 14.25, 33.55, 35.77, 4.84, 6.36]\n[41.5, 225.0, 218.66, 38.66, 128.33, 308.5]\n[20.27, 1.62, 22.67, 22.67, 33.44, 34.11]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped...\n218.66\n33.55\nsloped\nMULTIPOLYGON Z (((451697.087 4940060.556 49.62...\n\n\n306\n0.0\n0.0\n1360\nLidar\nLidar\n74.82\nNone\n20180530\n20180509\n54.55\n52.59\n182e58c1-67da-4cfa-af74-19cf4cdf5520\n52.68\n2.0\n2.0\n10.26\n1.25\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n98\n98-0\n2114\n2.2\n37\n1\n31\n5\n4\n1\n39.53\n52.68\n55.66\n63.63\n7.97\n496.63\n72.32\n343.98\n80.27\n72.78\n7.49\n657.93\n[72.32, 5.0, 8.59, 2.23, 0.24, 4.87, 20.81, 5....\n[0.0, 44.59, 128.48, 303.53, 58.06, 213.53, 30...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[5.0, 8.59, 2.23, 0.24, 4.87, 20.81, 5.05, 4.1...\n[44.59, 128.48, 303.53, 58.06, 213.53, 303.53,...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[23.37, 7.49, 8.04, 8.58, 32.86]\n[306.57, 315.0, 35.96, 216.5, 125.36]\n[22.76, 0.81, 38.93, 38.57, 22.45]\n['sloped', 'flat', 'sloped', 'sloped', 'sloped']\n216.50\n8.58\nsloped\nMULTIPOLYGON Z (((452061.691 4939653.73 52.68,...\n\n\n307\n0.0\n0.0\n1360\nLidar\nLidar\n95.90\nNone\n20180530\n20180509\n48.77\n48.35\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n48.40\n2.0\n2.0\n6.21\n1.65\nNone\n461\nMunicipal\nMunicipal\n23\nExcellent\nExcellent\nslanted\n1.0\n1.0\n99\n99-0\n2121\n2.2\n54\n1\n49\n4\n3\n1\n49.58\n48.40\n50.22\n56.48\n6.26\n463.31\n95.89\n250.01\n104.66\n84.18\n20.48\n480.60\n[95.9, 11.7, 0.81, 0.06, 0.29, 0.26, 1.73, 3.5...\n[0.0, 293.5, 126.07, 81.38, 199.89, 108.07, 17...\n[180.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90...\n[11.7, 0.81, 0.06, 0.29, 0.26, 1.73, 3.54, 0.6...\n[293.5, 126.07, 81.38, 199.89, 108.07, 17.53, ...\n[90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90....\n[4.1, 6.85, 85.96, 20.49]\n[305.54, 18.43, 199.23, 180.0]\n[9.96, 24.31, 27.1, 1.72]\n['sloped', 'sloped', 'sloped', 'flat']\n180.00\n20.49\nflat\nMULTIPOLYGON Z (((451634.297 4940024.668 48.4,...\n\n\n\n\n308 rows × 63 columns",
    "crumbs": [
      "Demo potentiel solaire -- Halifax"
    ]
  },
  {
    "objectID": "solar-potential.html#manually-adjust-heights-to-the-geoid-halifax",
    "href": "solar-potential.html#manually-adjust-heights-to-the-geoid-halifax",
    "title": "Demo potentiel solaire – Halifax",
    "section": "Manually adjust heights to the geoid @ Halifax",
    "text": "Manually adjust heights to the geoid @ Halifax\n\n# calc for Halifax : 20.447m\n# https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/tools-outils/gpsh.php\n\n# altitude according to PVGIS : 56 m\n\noffset_approx = 22.35  \noffset_website = 20.447  # The amount to subtract from all altitudes\noffset_PVGIS = 56  # The amount to subtract from all altitudes\n\noffset = offset_PVGIS # The amount to subtract from all altitudes\n\n# Apply the translation to all geometries\ngdf_LOD['geometry'] = gdf_LOD['geometry'].translate(xoff=0, yoff=0, zoff=-offset)\n\n\ngdf = gdf_LOD\n\n\npd.set_option('display.max_columns', None)\n#print(gdf.dtypes)\n\n\ngdf.head(50)\n\n\n# specify the columns you want to keep\ncolumns_to_keep = ['feature_id', 'bldgarea', \n                    'roof_surface_areas', 'roof_surface_azimuths', \n                    'roof_surface_inclinations', 'roof_surface_types', 'southern_azimuth', 'southern_area', 'southern_rooftype','geometry']\nlist1 = columns_to_keep\n\ncolumns_to_keep = ['attribute.feature_id', 'attribute.bldgarea', \n                   'attribute.southern_azimuth', 'attribute.southern_area', 'attribute.southern_rooftype', 'attribute.heightmax', 'geometry']\n\nlist2 = columns_to_keep\n\n\ndict1 = {item: item for item in list1}\n\n\ndict1\n\n\ndict2 = {}\nfor key in list1:\n    if f'attribute.{key}' in list2:\n        dict2[key] = f'attribute.{key}'\n    elif key in list2:  # For 'geometry' which doesn't have 'attribute.' prefix\n        dict2[key] = key\n    else:\n        dict2[key] = None\n\n\ndict2\n\n\n# drop columns that are not specified in the list\ngdf = gdf.drop(gdf.columns.drop(columns_to_keep), axis=1, errors='ignore')\n\n\ngdf.head()\n\n\n\n\n\n\n\n\nattribute.bldgarea\nattribute.feature_id\nattribute.heightmax\nattribute.southern_azimuth\nattribute.southern_area\nattribute.southern_rooftype\ngeometry\nattribute.southern_azimuth_transformed\nsolar_potential_index\nnumber_panels\n\n\n\n\n0\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 -5.21...\n141\n3\n4\n\n\n1\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451617.178 4940177.072 1.36,...\n141\n3\n4\n\n\n2\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.472 4940180.855 3.766...\n141\n3\n4\n\n\n3\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.737 4940180.686 -3.02...\n141\n3\n4\n\n\n4\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451607.514 4940169.24 -1.377...\n141\n3\n4\n\n\n\n\n\n\n\n\ndef transform_series_old(series):\n    \"\"\"\n    Transform values in a pandas Series from range [0, 360] to [0, 180].\n    With a maximum at 180, so \n    170 =&gt; 170\n    190 =&gt; 170\n    \n    Args:\n        series (pandas Series): A pandas Series of floating-point values\n    \n    Returns:\n        pandas Series: Transformed Series\n    \"\"\"\n    return series.where(series &lt;= 180, 360 - series)\n\ndef transform_series(series):\n    \"\"\"\n    Transform values in a pandas Series from range [0, 360] to [0, 180].\n    With a maximum at 180, so \n    170 =&gt; 170\n    190 =&gt; 170\n    \n    Args:\n        series (pandas Series): A pandas Series of floating-point values\n    \n    Returns:\n        pandas Series: Transformed Series\n    \"\"\"\n    # Convert series to float if it's not already\n    series = pd.to_numeric(series, errors='coerce')\n    # Round to nearest integer\n    series = series.round().astype(int)\n    \n    # Apply the transformation\n    return series.where(series &lt;= 180, 360 - series)\n\n\n# Function to calculate solar index\n\ndef calculate_solar_potential_old(df, minimum_area=1.65):\n    \"\"\"\n    Calculate the solar potential index and the number of panels for a given roof surface.\n\n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the roof surface data.\n    minimum_area (float, optional): The minimum area required for a roof surface to be considered for solar panels. Default is 1.65 square meters.\n\n    Returns:\n    None\n\n    \"\"\"\n      # Transform to azimuth max = 180 \n    df['attribute.southern_azimuth_transformed'] = df['attribute.southern_azimuth'].apply(lambda x: transform_series(pd.Series([x]))[0])\n    # Calculate the solar potential index\n\n    df['solar_potential_index'] = np.where(\n                                      (df['attribute.southern_rooftype'] == 'flat'), 5,\n                                  np.where(\n                                      (df['attribute.southern_area'] &lt; minimum_area) | (df['attribute.southern_azimuth_transformed'] &lt;= 90), 0,\n                                        np.round((df['attribute.southern_azimuth_transformed'] - 90) / 18).clip(0, 5)\n                                    )\n                                  ).astype(int)\n\n    df['number_panels'] = np.floor(df['attribute.southern_area'] / minimum_area).astype(int)\n\ndef calculate_solar_potential(df, minimum_area=1.65):\n    \"\"\"\n    Calculate the solar potential index and the number of panels for a given roof surface.\n\n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the roof surface data.\n    minimum_area (float, optional): The minimum area required for a roof surface to be considered for solar panels. Default is 1.65 square meters.\n\n    Returns:\n    None\n    \"\"\"\n    # Convert area to float\n    df['attribute.southern_area'] = pd.to_numeric(df['attribute.southern_area'], errors='coerce')\n\n    # Transform to azimuth max = 180 and convert to float\n    df['attribute.southern_azimuth_transformed'] = transform_series(df['attribute.southern_azimuth'])\n\n    # Calculate the solar potential index\n    df['solar_potential_index'] = np.where(\n        (df['attribute.southern_rooftype'] == 'flat'), 5,\n        np.where(\n            (df['attribute.southern_area'] &lt; minimum_area) | (df['attribute.southern_azimuth_transformed'] &lt;= 90), 0,\n            np.round((df['attribute.southern_azimuth_transformed'] - 90) / 18).clip(0, 5)\n        )\n    ).astype(int)\n\n    df['number_panels'] = np.floor(df['attribute.southern_area'] / minimum_area).astype(int)\n\n\ndef calculate_solar_azimuth_index(df, index_max=10):\n    \"\"\"\n    Calculate the solar azimuth index based on the southern azimuth.\n    \n    Parameters:\n    df (pandas DataFrame): The DataFrame containing the 'attribute.southern_azimuth' column.\n    index_max (int): The maximum value for the solar azimuth index. Default is 10.\n    \n    Returns:\n    None (modifies the DataFrame in place)\n    \"\"\"\n    \n    # Define the step size\n    step = 90 / index_max\n    \n    # Create the conditions and values for the index\n    conditions = [\n        (df['attribute.southern_azimuth'] &lt; 90) | (df['attribute.southern_azimuth'] &gt; 270),\n        (df['attribute.southern_azimuth'] &gt;= 180 - step/2) & (df['attribute.southern_azimuth'] &lt;= 180 + step/2)\n    ]\n    values = [0, index_max]\n    \n    # Add conditions and values for decreasing index on both sides of 180\n    for i in range(1, index_max):\n        lower_bound = 180 - (i+0.5)*step\n        upper_bound = 180 + (i+0.5)*step\n        conditions.append((df['attribute.southern_azimuth'] &gt;= lower_bound) & (df['attribute.southern_azimuth'] &lt; 180 - (i-0.5)*step))\n        conditions.append((df['attribute.southern_azimuth'] &gt; 180 + (i-0.5)*step) & (df['attribute.southern_azimuth'] &lt;= upper_bound))\n        values.extend([index_max - i, index_max - i])\n    \n    # Apply the conditions to create the solar_azimuth_index\n    df['solar_azimuth_index'] = np.select(conditions, values, default=0)\n\n# Usage\n# calculate_solar_azimuth_index(df, index_max=10)\n\n\ncalculate_solar_potential(gdf) # modifies gdf\n#calculate_solar_azimuth_index(gdf, index_max=10)",
    "crumbs": [
      "Demo potentiel solaire -- Halifax"
    ]
  },
  {
    "objectID": "solar-potential.html#arrow-table-stuff",
    "href": "solar-potential.html#arrow-table-stuff",
    "title": "Demo potentiel solaire – Halifax",
    "section": "Arrow table stuff",
    "text": "Arrow table stuff\n\ngdf_arrow = gdf.to_arrow()\n\n\ntype(gdf_arrow)\n\n\ngdf_arrow # The returned data object needs to be consumed by a library implementing the Arrow PyCapsule Protocol. For example, wrapping the data as a pyarrow.Table\n\n\ntable = pa.table(gdf_arrow)\n\n\ntable.column_names\n\n\ndef apply_operation(table, col1, col2, op):\n    # Convert ChunkedArrays to Python lists\n    list1 = table.column(col1).to_pylist()\n    list2 = table.column(col2).to_pylist()\n    \n    # Convert string representations to actual lists of floats\n    list1 = [ast.literal_eval(x) for x in list1]\n    list2 = [ast.literal_eval(x) for x in list2]\n    \n    # Apply the operation element-wise\n    result = []\n    for a, b in zip(list1, list2):\n        result.append([op(x, y) for x, y in zip(a, b)])\n    \n    # Convert the result back to a ChunkedArray\n    result_strings = [str(x) for x in result]\n    return pa.chunked_array([pa.array(result_strings)])\n\n\ndef string_to_float_list(s: str) -&gt; list:\n    return [float(x) for x in ast.literal_eval(s)]\n\ndef apply_operation_gen(col1: pa.ChunkedArray, col2: pa.ChunkedArray, op: Callable) -&gt; Iterator[str]:\n    for item1, item2 in zip(col1, col2):\n        list1 = string_to_float_list(item1.as_py())\n        list2 = string_to_float_list(item2.as_py())\n        result = [op(x, y) for x, y in zip(list1, list2)]\n        yield str(result)\n\ndef apply_operation_v2(table: pa.Table, col1_name: str, col2_name: str, op: Callable) -&gt; pa.ChunkedArray:\n    col1 = table.column(col1_name)\n    col2 = table.column(col2_name)\n    result_list = list(apply_operation_gen(col1, col2, op))\n    return pa.chunked_array([pa.array(result_list)])\n\n\nresult = apply_operation(table, 'roof_surface_azimuths', 'roof_surface_areas', operator.sub)\n\n\nresult_gen = apply_operation_v2(table, 'roof_surface_azimuths', 'roof_surface_areas', operator.sub)\n\n\nresult\n\n\ntable.column('roof_surface_azimuths')[0]",
    "crumbs": [
      "Demo potentiel solaire -- Halifax"
    ]
  },
  {
    "objectID": "solar-potential.html#gdf-stuff",
    "href": "solar-potential.html#gdf-stuff",
    "title": "Demo potentiel solaire – Halifax",
    "section": "GDF stuff",
    "text": "GDF stuff\n\ngdf\n\n\n\n\n\n\n\n\nattribute.bldgarea\nattribute.feature_id\nattribute.heightmax\nattribute.southern_azimuth\nattribute.southern_area\nattribute.southern_rooftype\ngeometry\nattribute.southern_azimuth_transformed\nsolar_potential_index\nnumber_panels\n\n\n\n\n0\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451615.812 4940179.385 -5.21...\n141\n3\n4\n\n\n1\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451617.178 4940177.072 1.36,...\n141\n3\n4\n\n\n2\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.472 4940180.855 3.766...\n141\n3\n4\n\n\n3\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451616.737 4940180.686 -3.02...\n141\n3\n4\n\n\n4\n83.24\n1ddd9cff-f4af-4254-b4cd-415fd1507fcd\n8.57\n219.04\n8.06\nsloped\nMULTIPOLYGON Z (((451607.514 4940169.24 -1.377...\n141\n3\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15809\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451637.207 4940027.548 -1.02...\n180\n5\n12\n\n\n15810\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451631.96 4940022.268 -5.274...\n180\n5\n12\n\n\n15811\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451638.405 4940030.285 -2.74...\n180\n5\n12\n\n\n15812\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451636.792 4940026.709 -1.50...\n180\n5\n12\n\n\n15813\n95.9\naf2342c1-6b7f-4e06-aeeb-97f1353bbd6c\n6.21\n180\n20.49\nflat\nMULTIPOLYGON Z (((451639.106 4940023.765 -5.75...\n180\n5\n12\n\n\n\n\n15814 rows × 10 columns\n\n\n\n\naz_index = gdf['attribute.southern_azimuth'].to_numpy()\naz_normalized = gdf['attribute.southern_azimuth_transformed'].to_numpy()\n\n\nSpectral_5.mpl_colormap\n#type(Reds_6)\n# RdBu_9.mpl_colormap\n\nSpectral  underbad over \n\n\n\ncolors = apply_continuous_cmap(normalized_heights, Spectral_5)\n\n\nheights = gdf['attribute.heightmax'].to_numpy()\nheights = np.nan_to_num(heights, nan=1)\n\n\nnormalizer = LogNorm(1, heights.max(), clip=True)\nnormalized_heights = normalizer(heights)\n# normalizer = LogNorm(1, roof_directions.max(), clip=True)\n# normalized_heights = normalizer(roof_directions)\n\n\n# colors = apply_continuous_cmap(az_normalized / 180, RdBu_9) # Reds_6)\ncolors = apply_continuous_cmap(az_index / 10, Reds_6) # Reds_6)\n\n\nprint(gdf['solar_azimuth_index'].value_counts())\n\n\nlayer25 = PolygonLayer.from_geopandas(\n    gdf,\n#    wireframe=True,\n#    extruded=True,\n#    filled=True,\n#    get_fill_color=colors,\n#    get_elevation=heights,\n#    auto_highlight=True,\n #   pickable=True,\n    #get_fill_color=np.flip(colors),\n)\n\n\nlayer3D = PolygonLayer.from_geopandas(\n    gdf_3D,\n#    wireframe=True,\n#    extruded=True,\n#    filled=True,\n#    get_fill_color=colors,\n#    get_elevation=heights,\n#    auto_highlight=True,\n #   pickable=True,\n    #get_fill_color=np.flip(colors),\n)\n\n\nflipped_colors = np.flip(custom_colors)\n\n\n#layerLOD =SolidPolygonLayer.from_geopandas(\nlayerLOD =PolygonLayer.from_geopandas(\n    gdf_LOD,\n    line_joint_rounded=True,\n#    elevation_scale = 0,\n#   wireframe=True,\n#    extruded=True,\n#    filled=True,\n#get_fill_color=Spectral_5,\n#    get_elevation=heights,\n#    auto_highlight=True,\n#   pickable=False,\n    #get_fill_color=np.flip(colors),\n)\n\n\nview_state_halifax = {\n    \"longitude\": -63.55, # Longitude at the map center\n    \"latitude\": 44.62, # Latitude at the map center\n    \"zoom\": 12.726630492730596,\n    \"pitch\": 0, # Pitch angle in degrees. `0` is top-down.\n    \"bearing\": 0, # Bearing angle in degrees. `0` is north.\n}\n\n\nlayer = PolygonLayer(\n    # Select only a few attribute columns from the table\n    # AttributeError: 'geoarrow.rust.core._rust.GeoTable' object [table_parquet_geoarrow_rust] has no attribute 'select'\n    table=table_pq.select([\"feature_id\", \"md_id\", \n                           \"acqtech\", \"acqtech_en\", \"acqtech_fr\",\n                           \"elevmin\", \"elevmax\", \"bldgarea\", \"comment\", \"geometry\"]),\n    #extruded=True,\n    wireframe=False,\n    extruded=False,\n    filled=False,\n    # stroked=False,\n    #get_elevation=heights,\n    #get_fill_color=colors,\n)\n\n\nm = Map(layerLOD,  _height=800)\n#m = Map(layer, view_state=view_state_halifax, _height=800)\n\n\nm",
    "crumbs": [
      "Demo potentiel solaire -- Halifax"
    ]
  },
  {
    "objectID": "cmcade-read-delta.html",
    "href": "cmcade-read-delta.html",
    "title": "CMC ADE – read data from Delta Lake tables",
    "section": "",
    "text": "import datetime as dt\nfrom datetime import date\nimport os\nimport pathlib\nimport tarfile\nimport time\nimport daft\nimport deltalake\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport polars.selectors as cs\nimport pyarrow as pa\nfrom deltalake import DeltaTable  # S3FileSystem ??\nfrom deltalake.writer import write_deltalake\nfrom tabulate import tabulate\nfrom tqdm.notebook import tqdm\n# opt.maxBytes = 131072\nimport matplotlib.pyplot as plt\nimport hvplot.pandas # Won't be needed hopefully; we'll be using Polars\nimport hvplot.polars\nimport seaborn as sns\nimport seaborn.objects as so\nimport geopandas as gpd\nfrom shapely.geometry import Point\n# hvplot.extension(\"plotly\")\n\n\nfrom great_tables import GT, md, html, style, loc\nfrom great_tables.data import airquality, islands\n\nEN Open the Delta Table with Polars and test the various Delta tables (see CMCADE-ingest.ipynb) - tar_swob_no_optimization : dataframes written (then appended) without partitioning\nFR À venir\n\ndelta_plain = pl.scan_delta(\"tar_swob_no_optimization\").collect()\nbad_data = delta_plain.count()['name'] - delta_plain.count()['value']\nmin_date = delta_plain['date_tm'].min()\nmax_date = delta_plain['date_tm'].max()\n\nprint(f\"Minimum Date: {min_date}\")\nprint(f\"Maximum Date: {max_date}\")\n\n\nscan_delta_plain = pl.scan_delta(\"tar_swob_no_optimization\")\nscan_delta_by_stn_plain = pl.scan_delta(\"tar_swob_P_by_stn_name\")\n\nstorage_options = {\"partition_by\": \"stn_nam\"}\nscan_delta_by_stn_arg = pl.scan_delta(\"tar_swob_P_by_stn_name\", storage_options=storage_options)\n\n\ndelta_plain\n\n\n# pl_airquality = pl.DataFrame(airquality_mini).select(\n#     \"Year\", \"Month\", \"Day\", \"Ozone\", \"Solar_R\", \"Wind\", \"Temp\"\n# )\nbad_values = str(bad_data[0])\ngt_air = GT(delta_plain.count())\n(\n    gt_air\n    .fmt_integer(columns = delta_plain.columns, sep_mark=\" \",)\n    \n    # Table header ----\n    .tab_header(\n        title = \"CMC weather observations data counts\",\n        subtitle = f\"Between {min_date} and {max_date} (inclusive); Bad data values = {bad_values}\"\n    )\n    \n    # Table styles ----\n    .tab_style(\n        style.fill(\"lightgray\"),\n        loc.body(\n            columns = cs.all()\n        )\n    )\n\n)\n\n\nFiltering\nJust as easy as an Excel formula :-)\n\n# Your existing query\n# Define the start and end dates; will be used for the graph title\nstart_date = date(2024, 3, 4)\nend_date = date(2024, 3, 25)\n\nbasic_query_read = (\n    #pl.read_delta(\"tar_swob_no_optimization\") # NO PLANNING AHEAD\n    pl.read_delta(\"tar_swob_P_by_stn_name\") # NO PLANNING AHEAD\n    .filter(pl.col(\"name\") == \"air_temp\")\n#    .filter(pl.col(\"stn_nam\") == \"STE-FOY (U. LAVAL)\")\n    .filter(pl.col(\"stn_nam\").is_in([\"STE-FOY (U. LAVAL)\", \"MALAHAT\", \"ABEE AGDM\", \"ALDERSVILLE\"]))\n#  .filter(pl.col(\"stn_nam\").is_in(stn_nam_list))\n    .filter(pl.col(\"date_tm\").is_between(start_date, end_date))\n    .sort(pl.col([\"date_tm\",\"stn_nam\"]), descending=False)\n#    .with_column(\n#        (pl.col(\"value\").max() - pl.col(\"value\").mean()).alias(\"difference\"))\n)\n\n\n# Your existing query\n# Define the start and end dates; will be used for the graph title\nstart_date = date(2024, 3, 4)\nend_date = date(2024, 3, 25)\n\nstorage_options = {\"partition_by\": \"stn_nam\"}\n\nbasic_query_read = (\n    #pl.scan_delta(\"tar_swob_P_by_stn_name\") \n    pl.scan_delta(\"tar_swob_P_by_stn_name\", storage_options=storage_options)\n    .filter(pl.col(\"name\") == \"air_temp\")\n    .filter(pl.col(\"stn_nam\") == \"STE-FOY (U. LAVAL)\")\n#   .filter(pl.col(\"stn_nam\").is_in([\"STE-FOY (U. LAVAL)\", \"MALAHAT\", \"ABEE AGDM\", \"ALDERSVILLE\"]))\n#  .filter(pl.col(\"stn_nam\").is_in(stn_nam_list))\n    .filter(pl.col(\"date_tm\").is_between(start_date, end_date))\n    .sort(pl.col([\"date_tm\",\"stn_nam\"]), descending=False)\n    .collect()\n#    .with_column(\n#        (pl.col(\"value\").max() - pl.col(\"value\").mean()).alias(\"difference\"))\n)\n\n\nbasic_query_read\n\n\nscan_delta_plain = pl.scan_delta(\"tar_swob_no_optimization\")\nscan_delta_by_stn_plain = pl.scan_delta(\"tar_swob_P_by_stn_name\")\n\nstorage_options = {\"partition_by\": \"stn_nam\"}\nscan_delta_by_stn_arg = pl.scan_delta(\"tar_swob_P_by_stn_name\", storage_options=storage_options)\n\n\n\nAggregating\nAdd columns dynamically\n\n# Add a column showing the difference between the max and mean over a time period\nstart_date = date(2024, 3, 4)\nend_date = date(2024, 3, 25)\n\nbasic_query_read_ag = (\n    pl.read_delta(\"tar_swob_no_optimization\")\n    .filter(pl.col(\"name\") == \"air_temp\")\n   .filter(pl.col(\"stn_nam\") == \"STE-FOY (U. LAVAL)\")\n#    .filter(pl.col(\"stn_nam\").is_in([\"STE-FOY (U. LAVAL)\", \"MALAHAT\", \"ABEE AGDM\", \"ALDERSVILLE\"]))\n#  .filter(pl.col(\"stn_nam\").is_in(stn_nam_list))\n    .filter(pl.col(\"date_tm\").is_between(start_date, end_date))\n    .sort(pl.col([\"date_tm\",\"stn_nam\"]), descending=False)\n    .select(pl.col(\"value\").mean())\n#    .with_column(\n#        (pl.col(\"value\").max() - pl.col(\"value\").mean()).alias(\"difference\"))\n)\n\n\nbasic_query_read_ag\n\n\n\nPower of Polars\nCase of parquet, but even better on Delta Lake tables\n\nquery = (\n    pl.scan_parquet(\"yellow_tripdata_2023-01.parquet\")\n    .join(pl.scan_csv(\"taxi_zone_lookup.csv\"), left_on=\"PULocationID\", right_on=\"LocationID\")\n    .filter(pl.col(\"total_amount\") &gt; 25)\n    .group_by(\"Zone\")\n    .agg(\n        (pl.col(\"total_amount\") /\n        (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).dt.total_minutes()\n        ).mean().alias(\"cost_per_minute\")\n    ).sort(\"cost_per_minute\",descending=True)\n)\n\n\nquery.collect()\n\n\nbasic_query_read_ag\n\n\n\nPlotting\n\n# Plot the DataFrame using hvplot\nhvplot.extension(\"bokeh\")\n# hvplot.extension(\"plotly\")\n# hvplot.extension(\"matplotlib\")\nplot = basic_query_read.hvplot.line(x='date_tm', \n                                    y='value', \n                                    by='stn_nam', \n                                    title=f\"Air Temperature between {start_date} and {end_date}\")\n\n# Set the y-axis label to include the 'uom' value\n# Assuming 'uom' is a constant value for all rows in your filtered DataFrame\nuom = \"°C\" # This should be dynamically fetched if it varies\nplot.opts(ylabel=f\"Temperature ({uom})\")\nplot.opts(xlabel=f\"Date J/MM\")\nplot\n\n\n\nAggregating\n\n# Plot the DataFrame using hvplot\nhvplot.extension(\"bokeh\")\n# hvplot.extension(\"plotly\")\n# hvplot.extension(\"matplotlib\")\nplot = basic_query_read.hvplot.line(x='date_tm', \n                                    y='value', \n                                    by='stn_nam', \n                                    title=f\"Air Temperature between {start_date} and {end_date}\")\n\n# Set the y-axis label to include the 'uom' value\n# Assuming 'uom' is a constant value for all rows in your filtered DataFrame\nuom = \"°C\" # This should be dynamically fetched if it varies\nplot.opts(ylabel=f\"Temperature ({uom})\")\nplot.opts(xlabel=f\"Date J/MM\")\nplot\n\n\n\nGeo use case\nIf instead of specifying a list of station names we had a map that would allow selecting stations by some geo-specific query, e.g. all stations within an arbitrary polygon ?\nWe’ll keep this example simple. Since we have data in degrees I will ask for all stations that fall within a radius of a point. In my case, the point will be the city of Vancouver.\nIn a real world application we would propose the user with a way to supply a range in a decent distance dimension (e.g. kilometers)\nWhat we want is to replace the list in\n.filter(pl.col(\"stn_nam\").is_in([\"STE-FOY (U. LAVAL)\", \"MALAHAT\", \"ABEE AGDM\", \"ALDERSVILLE\"]))\nabove with a list of names coming from our buffer (see below)\n[\"DELTA BURNS BOG\", \"POINT ATKINSON\", \"VANCOUVER HARBOUR CS\", \"VANCOUVER SEA ISLAND CCG\", \"WEST VANCOUVER AUT\"]\n\n\nCreate geo dataframe\nWe first take the first occurrence of each station name and corresponding lat, long\nWe could save as GeoJSON, but for this notebook we’ll just keep the geo dataframe\n\nstn_loc_df = delta_plain.unique(subset=['stn_nam'], keep='first').sort(\"stn_nam\").select(['stn_nam', 'lat', 'long'])\n\n\nstn_loc_df\n\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(stn_loc_df['long'], stn_loc_df['lat'])]\n\n#geo_df = gpd.GeoDataFrame(stn_loc_df, geometry=geometry)\ngeo_df = gpd.GeoDataFrame(stn_loc_df[['stn_nam']], geometry=geometry)\n\n# Assuming geo_df is your GeoDataFrame\ngeo_df.rename(columns={0: 'stn_nam'}, inplace=True)\n\n\n# Save as GeoJSON\n# geo_df.to_file(\"obs-stations.geojson\", driver=\"GeoJSON\")\n\n\ngeo_df\n\n\n\nDetermine a point of interest and find all stations within a radius\n\n# Assuming geo_df is your GeoDataFrame with 'stn_nam' and 'geometry' columns\n# Define the point of interest (latitude, longitude)\n# In our case, Vancouver -123.1139456, 49.2604134\npoint_of_interest = Point(-123.1139456, 49.2604134)\n\n# Create a buffer around the point of interest in degrees\n# Note: This is a simplification and might not accurately represent a real-world distance\nbuffer_distance_in_degrees = 0.2 # Example buffer distance in degrees\nbuffer = point_of_interest.buffer(buffer_distance_in_degrees)\n\n# Convert the buffer to a GeoDataFrame\nbuffer_gdf = gpd.GeoDataFrame(geometry=[buffer], crs=geo_df.crs)\n\n# Perform a spatial join to find all stations within the buffer\nstations_within_buffer = gpd.sjoin(geo_df, buffer_gdf, how='inner', predicate='within')\n\n# Print the stations within the buffer\nprint(stations_within_buffer)\n\n\n# Derive the list of station names from the geo dataframe and visualize as per above\nstn_nam_list = stations_within_buffer['stn_nam'].values.tolist()\n\n# Print the list of station names\nprint(stn_nam_list)",
    "crumbs": [
      "CMC ADE -- read data from Delta Lake tables"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "geopackage_to_delta.html",
    "href": "geopackage_to_delta.html",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Existing code handles downloading and extracting GeoPackages\nCan read individual layers and compare schemas\n\n\n\n\n\nCreate function to read and concatenate GeoDataFrames\n\nHandle multiple files efficiently\nTrack progress for large operations\n\nCoordinate System Management\n\nEnsure consistent CRS across all data\nTransform coordinates if needed\n\nError Handling\n\nTrack failed reads\nLog issues with specific files\n\n\n\n\n\n\n\n\n\nGeometry Conversion\n\nConvert geometries to WKT or WKB format\nEnsure format compatibility with Delta Lake\n\n\n\n\n\n\nColumn Standardization\n\nEnsure consistent data types\nHandle missing values\nValidate data integrity\n\n\n\n\n\n\nAdd tracking columns:\n\nSource file identifier\nProcessing timestamp\nOriginal CRS\nData quality indicators\n\n\n\n\n\n\n\n\n\nSchema Definition\n\nDefine table structure\nConsider partitioning strategy\nPlan for spatial indexing\n\n\n\n\n\n\nLocal Setup\n\nDefine storage location\nConfigure Delta Lake properties\nSet up table optimization parameters\n\n\n\n\n\n\n\n\n\nPrerequisites\n\nInstall and configure obstore\nSet up Azure credentials\n\nImplementation\n\nConfigure blob storage connection\nMaintain Delta Lake directory structure\nHandle large file transfers\n\n\n\n\n\n\nSetup\n\nConfigure Azure connection in Cursor\nVerify access permissions\n\nImplementation\n\nUse built-in Azure operations\nHandle directory uploads\nMaintain file relationships\n\n\n\n\n\n\n\nStart with Phase 1 implementation\nTest data consolidation thoroughly\nImplement Delta Lake conversion\nUse Cursor Azure Plugin for initial cloud storage tests\nConsider obstore if more advanced blob storage operations are needed\n\n\n\n\n\nEach phase should include proper error handling and logging\nConsider implementing progress tracking for long-running operations\nTest with small subset before processing full dataset\nDocument any schema or data modifications"
  },
  {
    "objectID": "geopackage_to_delta.html#phase-1-data-collection-and-consolidation",
    "href": "geopackage_to_delta.html#phase-1-data-collection-and-consolidation",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Existing code handles downloading and extracting GeoPackages\nCan read individual layers and compare schemas\n\n\n\n\n\nCreate function to read and concatenate GeoDataFrames\n\nHandle multiple files efficiently\nTrack progress for large operations\n\nCoordinate System Management\n\nEnsure consistent CRS across all data\nTransform coordinates if needed\n\nError Handling\n\nTrack failed reads\nLog issues with specific files"
  },
  {
    "objectID": "geopackage_to_delta.html#phase-2-data-preparation-for-delta-lake",
    "href": "geopackage_to_delta.html#phase-2-data-preparation-for-delta-lake",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Geometry Conversion\n\nConvert geometries to WKT or WKB format\nEnsure format compatibility with Delta Lake\n\n\n\n\n\n\nColumn Standardization\n\nEnsure consistent data types\nHandle missing values\nValidate data integrity\n\n\n\n\n\n\nAdd tracking columns:\n\nSource file identifier\nProcessing timestamp\nOriginal CRS\nData quality indicators"
  },
  {
    "objectID": "geopackage_to_delta.html#phase-3-delta-lake-table-creation",
    "href": "geopackage_to_delta.html#phase-3-delta-lake-table-creation",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Schema Definition\n\nDefine table structure\nConsider partitioning strategy\nPlan for spatial indexing\n\n\n\n\n\n\nLocal Setup\n\nDefine storage location\nConfigure Delta Lake properties\nSet up table optimization parameters"
  },
  {
    "objectID": "geopackage_to_delta.html#phase-4-cloud-storage-upload",
    "href": "geopackage_to_delta.html#phase-4-cloud-storage-upload",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Prerequisites\n\nInstall and configure obstore\nSet up Azure credentials\n\nImplementation\n\nConfigure blob storage connection\nMaintain Delta Lake directory structure\nHandle large file transfers\n\n\n\n\n\n\nSetup\n\nConfigure Azure connection in Cursor\nVerify access permissions\n\nImplementation\n\nUse built-in Azure operations\nHandle directory uploads\nMaintain file relationships"
  },
  {
    "objectID": "geopackage_to_delta.html#recommended-approach",
    "href": "geopackage_to_delta.html#recommended-approach",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Start with Phase 1 implementation\nTest data consolidation thoroughly\nImplement Delta Lake conversion\nUse Cursor Azure Plugin for initial cloud storage tests\nConsider obstore if more advanced blob storage operations are needed"
  },
  {
    "objectID": "geopackage_to_delta.html#notes",
    "href": "geopackage_to_delta.html#notes",
    "title": "GeoPackage to Delta Lake Implementation Plan",
    "section": "",
    "text": "Each phase should include proper error handling and logging\nConsider implementing progress tracking for long-running operations\nTest with small subset before processing full dataset\nDocument any schema or data modifications"
  },
  {
    "objectID": "solar-gc.html",
    "href": "solar-gc.html",
    "title": "Cleanup",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport ast\nimport operator\nfrom typing import Callable, Iterator\n\nimport pyarrow.parquet as pq\nimport pyarrow.compute as pc\nimport overturemaps\nfrom palettable.colorbrewer.sequential import Reds_6\nfrom palettable.colorbrewer.diverging import RdBu_9\nfrom pathlib import Path\nimport polars as pl\nfrom deltalake import DeltaTable, write_deltalake\nimport ast\nimport lonboard\nfrom lonboard import Map, PolygonLayer, SolidPolygonLayer, viz\nfrom lonboard.colormap import apply_continuous_cmap\nimport geoarrow.rust.core\nfrom geoarrow.rust.io import read_parquet\nimport bokeh, ipyleaflet\n\nimport geopandas as gpd\nimport leafmap\nfrom matplotlib.colors import LogNorm\n# import leafmap.deckgl as leafmap\nimport polars.selectors as cs\nfrom great_tables import loc, style, GT",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#wrong-encoding",
    "href": "solar-gc.html#wrong-encoding",
    "title": "Cleanup",
    "section": "Wrong encoding",
    "text": "Wrong encoding\nBecause the encoding is not specified, software will expect UTF-8 but it is not. This is a hurdle for interoperability purposes.\n\ndef convert_files_to_utf8(files):\n    for file in files:\n        # Open the CSV file with Latin1 encoding\n        df = pd.read_csv(file, encoding='latin1')\n        \n        # Save the DataFrame to a new CSV file with UTF-8 encoding\n        new_file = file.replace('.csv', '_utf8.csv')\n        df.to_csv(new_file, encoding='utf-8', index=False)\n\n\nfiles_to_convert = [\"solar-gc/municip_potentiel-potential.csv\", \n                    \"solar-gc/municip_kWh.csv\", \n                    \"solar-gc/municip_MJ.csv\"]\n\n\nconvert_files_to_utf8(files_to_convert)",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#empty-columns",
    "href": "solar-gc.html#empty-columns",
    "title": "Cleanup",
    "section": "Empty columns",
    "text": "Empty columns\nmunicip_potentiel-potential.csv has 10 empty columns !\n\nproblem_file = Path('solar-gc/municip_potentiel-potential_utf8.csv')\n\n\n# Read the CSV file\ndf = pd.read_csv(problem_file)\n\n\ndf.head()\n\n\n# Drop columns with all NaN values\ndf2 = df.dropna(axis=1, how='all')\n\n\ndf2.head()\n\n\n# Write the updated DataFrame to a new CSV file\ndf2.to_csv(problem_file, index=False)\n# Half a MB gone ...",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#exceedingly-complex-header",
    "href": "solar-gc.html#exceedingly-complex-header",
    "title": "Cleanup",
    "section": "Exceedingly complex header",
    "text": "Exceedingly complex header\n\nthree lines (instead of one); all files : municip_potentiel-potential.csv, municip_kWh.csv, municip_MJ.csv\nbilingual column names\n\nAs a result, the first three columns are unnamed because their first row is empty …\nSchemas need to be in English only. Translations should be managed elsewhere, not in the dataset. Plus, bilingual column names mean ludicrous/unmanageable column names.\nThis is a MAJOR hurdle for interoperability purposes. See below.\n\nfrom IPython.display import Image\n\n\nImage(filename='./solar-gc/malformed_header.png')\n\n\nfrom io import StringIO\n\ndef concatenate_columns(csv_data):\n    # Read the CSV data\n    df = pd.read_csv(StringIO(csv_data))\n\n    # Apply a lambda function to concatenate strings in each row\n    df['concatenated'] = df.apply(lambda row: ''.join(map(str, row.values)), axis=1)\n\n    return df['concatenated']\n\n\ncsv_data_MJ = \"\"\"\nUnnamed: 0,Unnamed: 1,Unnamed: 2,Mean daily global insolation (MJ/m2),Mean daily global insolation (MJ/m2).1,Mean daily global insolation (MJ/m2).2,Mean daily global insolation (MJ/m2).3,Mean daily global insolation (MJ/m2).4,Mean daily global insolation (MJ/m2).5\n,,,Ensoleillement global quotidien moyen (MJ/m2),Ensoleillement global quotidien moyen (MJ/m2),Ensoleillement global quotidien moyen (MJ/m2),Ensoleillement global quotidien moyen (MJ/m2),Ensoleillement global quotidien moyen (MJ/m2),Ensoleillement global quotidien moyen (MJ/m2)\nProvince,Municipality,Mois,South-facing with vertical (90 degrees) tilt,South-facing with latitude tilt,South-facing with tilt=latitude+15 degrees,South-facing with tilt=latitude-15 degrees,2-axis tracking,Horizontal (0 degree)\nProvince,Municipalité,Month,Vertical orienté vers le sud (inclinaison=90 degrés),Orienté vers le sud avec inclinaison=latitude,Orienté vers le sud avec inclinaison=latitude+15 degrés,Orienté vers le sud avec inclinaison=latitude-15 degrés,Suivi du soleil selon deux axes,Horizontal (inclinaison=0 degré)\n\"\"\"\n\n\ncsv_data_MPV = \"\"\"Unnamed: 0,Unnamed: 1,Unnamed: 2,Photovoltaic potential (kWh/kWp),Photovoltaic potential (kWh/kWp).1,Photovoltaic potential (kWh/kWp).2,Photovoltaic potential (kWh/kWp).3,Photovoltaic potential (kWh/kWp).4,Photovoltaic potential (kWh/kWp).5\n,,,Potentiel photovoltaïque (kWh/kWp),Potentiel photovoltaïque (kWh/kWp),Potentiel photovoltaïque (kWh/kWp),Potentiel photovoltaïque (kWh/kWp),Potentiel photovoltaïque (kWh/kWp),Potentiel photovoltaïque (kWh/kWp)\nProvince,Municipality,Mois,South-facing with vertical (90 degrees) tilt,South-facing with latitude tilt,South-facing with tilt=latitude+15 degrees,South-facing with tilt=latitude-15 degrees,2-axis tracking,Horizontal (0 degree)\nProvince,Municipalité,Month,Vertical orienté vers le sud (inclinaison=90 degrés),Orienté vers le sud avec inclinaison=latitude,Orienté vers le sud avec inclinaison=latitude+15 degrés,Orienté vers le sud avec inclinaison=latitude-15 degrés,Suivi du soleil selon deux axes,Horizontal (inclinaison=0 degré)\n\"\"\"\n\n\n# Read the CSV data\ndf = pd.read_csv(StringIO(csv_data_MPV), header=None)\n# Initialize a list to store the concatenated columns\nconcatenated_columns = []\n\n\n# Loop through each column and concatenate its values\nfor col in df.columns:\n    column_values = df[col].fillna('').astype(str).tolist()\n    concatenated_column = ''.join(column_values)\n    concatenated_columns.append(concatenated_column)\n\n\n# Print the concatenated columns\nfor i, column in enumerate(concatenated_columns):\n    print(f\"Column {i}: {column}\")\n\n\n# Check the number of rows\nprint(\"Number of rows:\", len(df))\n\n\n# Concatenate each column\nconcatenated_columns = df.apply(lambda x: ''.join(map(str, x.fillna(''))), axis=0)\n\n\n# Check the concatenated column\nprint(df['concatenated'])\n\n\nconcatenated_column = concatenate_columns(csv_data_MJ)\nlen(concatenated_column)\nconcatenated_column",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#replace-header-row-and-delete-first-three-rows",
    "href": "solar-gc.html#replace-header-row-and-delete-first-three-rows",
    "title": "Cleanup",
    "section": "Replace header row and delete first three rows",
    "text": "Replace header row and delete first three rows\n\npv_file = Path('solar-gc/municip_potentiel-potential_utf8.csv')\nkwh_file = Path('solar-gc/municip_kWh_utf8.csv')\nmj_file = Path('solar-gc/municip_MJ_utf8.csv')\n\n\n# Read the CSV file\ndf_pv = pd.read_csv(pv_file)\ndf_kwh = pd.read_csv(kwh_file)\ndf_mj = pd.read_csv(mj_file)\n\n\ndf_kwh.head()\n\n\nnew_column_names_mj = ['Province','Municipality','Month','MDGI_mj_south_vert_tilt','MDGI_mj_south_lat_tilt','MDGI_mj_south_lat_plus_15_tilt','MDGI_mj_south_lat_minus_15_tilt','MDGI_mj_two_axis_tracking','MDGI_mj_hor']\n\nnew_column_names_kwh = ['Province','Municipality','Month','MDGI_kwh_south_vert_tilt','MDGI_kwh_south_lat_tilt','MDGI_kwh_south_lat_plus_15_tilt','MDGI_kwh_south_lat_minus_15_tilt','MDGI_kwh_two_axis_tracking','MDGI_kwh_hor']\n\nnew_column_names_pv = ['Province','Municipality','Month','PV_pot_kWh_p_south_vert_tilt','PV_pot_kWh_p_south_lat_tilt','PV_pot_kWh_p_south_lat_plus_15_tilt','PV_pot_kWh_p_south_lat_minus_15_tilt','PV_pot_kWh_p_two_axis_tracking','PV_pot_kWh_p_hor']\n\n\ndf_pv.columns = new_column_names_pv\ndf_kwh.columns = new_column_names_kwh\ndf_mj.columns = new_column_names_mj\n\n\n# remove first three rows\ndf_pv = df_pv[3:]\ndf_kwh = df_kwh[3:]\ndf_mj = df_mj[3:]",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#more-string-manipulation",
    "href": "solar-gc.html#more-string-manipulation",
    "title": "Cleanup",
    "section": "More string manipulation",
    "text": "More string manipulation\nProvince and month\n\nselected_columns = ['Province','Month']\nfor column in selected_columns:\n    unique_values = df_kwh[column].unique()\n    print(f\"Unique values in column '{column}': {unique_values}\")\n\n\nprovince_map = {\n    'Alberta/Alberta': 'ALTA',\n    'British Columbia/Colombie-Britannique': 'BC',\n    'Manitoba/Manitoba': 'MAN',\n    'New Brunswick/Nouveau-Brunswick': 'NB',\n    'Newfoundland and Labrador/Terre-Neuve-et-Labrador': 'NFL',\n    'Northwest Territories/Territoires du Nord-Ouest': 'NWT',\n    'Nova Scotia/Nouvelle-Écosse': 'NS',\n    'Nunavut/Nunavut': 'NU',\n    'Ontario/Ontario': 'ON',\n    'Prince Edward Island/île du Prince-Édouard': 'PEI',\n    'Quebec/Québec': 'QC',\n    'Saskatchewan/Saskatchewan': 'SK',\n    'Yukon Territory/Yukon': 'YK'\n}\n\n\nmonth_map = {\n    'January/Janvier': 'Jan',\n    'February/Février': 'Feb',\n    'March/Mars': 'Mar',\n    'April/Avril': 'Apr',\n    'May/Mai': 'May',\n    'June/Juin': 'Jun',\n    'July/Juillet': 'Jul',\n    'August/Août': 'Aug',\n    'September/Septembre': 'Sep',\n    'October/Octobre': 'Oct',\n    'November/Novembre': 'Nov',\n    'December/Décembre': 'Dec',\n    'Annual/Annuel': 'Annual'\n}\n\n\ndataframes = [df_pv, df_mj, df_kwh]\n\n\ndf_pv['Province'] = df_pv['Province'].map(province_map)\n\n\ndef replace_values(df):\n    df['Province'] = df['Province'].apply(lambda x: province_map.get(x, x))\n    df['Month'] = df['Month'].apply(lambda x: month_map.get(x, x))\n    return df\n\n\n# Apply the function to each DataFrame in the list\nfor i, df in enumerate(dataframes):\n    dataframes[i] = replace_values(df)\n\n# Print the updated DataFrames\n#for df in dataframes:\n#    print(df)",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#save-clean-files-to-delta-lake",
    "href": "solar-gc.html#save-clean-files-to-delta-lake",
    "title": "Cleanup",
    "section": "Save clean files to delta lake",
    "text": "Save clean files to delta lake",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "solar-gc.html#unnecessary-information",
    "href": "solar-gc.html#unnecessary-information",
    "title": "Cleanup",
    "section": "Unnecessary information",
    "text": "Unnecessary information\nMJ/m² et kWh/m² : No need for two files since it is just a conversion factor !!\n1 kWh/m² = 3.597122 mJ/m²\n1 MJ/m² = 0.278 kWh/m²\nEvery 12 rows : Annual/Annuel; - not necessary (can be computed) - is misleading! should be “Annual mean”",
    "crumbs": [
      "Cleanup"
    ]
  },
  {
    "objectID": "geoai_buildings.html",
    "href": "geoai_buildings.html",
    "title": "GeoAI buildings extraction",
    "section": "",
    "text": "import polars as pl\nimport importlib\nimport my_nb_utils\nimportlib.reload(my_nb_utils)\n\n&lt;module 'my_nb_utils' from 'c:\\\\Users\\\\ymoisan\\\\github\\\\nbcmc\\\\nbs\\\\my_nb_utils.py'&gt;\n\n\n\nfrom my_nb_utils import (\n    get_gpkg_schema,\n    compare_gpkg_schemas,\n    print_schema_comparison,\n    read_and_concat_gpkgs,\n    prepare_gdf_for_delta,\n    list_gpkg_files,\n    get_all_gpkg_data,\n    write_gdf_to_delta,\n    save_processed_data,\n    load_processed_data,\n    to_polars,\n    get_building_stats,\n    analyze_overlaps\n    \n)\n\n\nGet all building footprints from FTP site\n\ndf, stats = get_all_gpkg_data()\n\nConnecting to FTP server...\n\nNavigating to GPKG directory...\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[46], line 1\n----&gt; 1 df, stats = get_all_gpkg_data()\n\nFile c:\\Users\\ymoisan\\github\\nbcmc\\nbs\\my_nb_utils.py:680, in get_all_gpkg_data(layer, target_crs)\n    677 start_time = datetime.now()\n    679 # Get list of files with timestamps\n--&gt; 680 files_info = list_gpkg_files()\n    682 if not files_info:\n    683     raise ValueError(\"No files found on FTP server\")\n\nFile c:\\Users\\ymoisan\\github\\nbcmc\\nbs\\my_nb_utils.py:609, in list_gpkg_files()\n    606 ftp.cwd('/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/')\n    608 # Get list of zip files\n--&gt; 609 filenames = ftp.nlst()\n    610 zip_files = [f for f in filenames if f.endswith('.zip')]\n    612 # Get detailed info for each file\n\nFile c:\\Users\\ymoisan\\AppData\\Local\\miniforge3\\envs\\twins\\Lib\\ftplib.py:547, in FTP.nlst(self, *args)\n    545     cmd = cmd + (' ' + arg)\n    546 files = []\n--&gt; 547 self.retrlines(cmd, files.append)\n    548 return files\n\nFile c:\\Users\\ymoisan\\AppData\\Local\\miniforge3\\envs\\twins\\Lib\\ftplib.py:462, in FTP.retrlines(self, cmd, callback)\n    459 with self.transfercmd(cmd) as conn, \\\n    460          conn.makefile('r', encoding=self.encoding) as fp:\n    461     while 1:\n--&gt; 462         line = fp.readline(self.maxline + 1)\n    463         if len(line) &gt; self.maxline:\n    464             raise Error(\"got more than %d bytes\" % self.maxline)\n\nFile c:\\Users\\ymoisan\\AppData\\Local\\miniforge3\\envs\\twins\\Lib\\socket.py:708, in SocketIO.readinto(self, b)\n    706 while True:\n    707     try:\n--&gt; 708         return self._sock.recv_into(b)\n    709     except timeout:\n    710         self._timeout_occurred = True\n\nKeyboardInterrupt: \n\n\n\n\ntype(df)\ndf\n\n\n\n\n\n\n\n\nbuil_fid\nsubproj_id\ngeometry\nsource_file\nprovince_code\nprocessing_timestamp\noriginal_crs\narea\nperimeter\nupload_timestamp\nquality_comments\ngeometry_crs\nhas_valid_geometry\n\n\n\n\n0\n5890242\n497\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1d\\x00...\nPE_Stracthcona_GE01_20221103.zip\nPE\n2025-01-14 15:55:17.928381\nEPSG:4617\nNaN\nNaN\n2024-10-21 14:26:05\nNaN in file for area; NaN in file for perimeter\nEPSG:4326\nTrue\n\n\n1\n5890243\n497\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00#\\x00\\x0...\nPE_Stracthcona_GE01_20221103.zip\nPE\n2025-01-14 15:55:17.928381\nEPSG:4617\nNaN\nNaN\n2024-10-21 14:26:05\nNaN in file for area; NaN in file for perimeter\nEPSG:4326\nTrue\n\n\n2\n5890244\n497\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x17\\x00...\nPE_Stracthcona_GE01_20221103.zip\nPE\n2025-01-14 15:55:17.928381\nEPSG:4617\nNaN\nNaN\n2024-10-21 14:26:05\nNaN in file for area; NaN in file for perimeter\nEPSG:4326\nTrue\n\n\n3\n5890245\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x17\\x00...\nPE_Stracthcona_GE01_20221103.zip\nPE\n2025-01-14 15:55:17.928381\nEPSG:4617\nNaN\nNaN\n2024-10-21 14:26:05\nNaN in file for area; NaN in file for perimeter\nEPSG:4326\nTrue\n\n\n4\n5890285\n497\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00)\\x00\\x0...\nPE_Stracthcona_GE01_20221103.zip\nPE\n2025-01-14 15:55:17.928381\nEPSG:4617\nNaN\nNaN\n2024-10-21 14:26:05\nNaN in file for area; NaN in file for perimeter\nEPSG:4326\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3230085\n4200925\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x00...\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n53.61\n35.21\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n3230086\n4200926\n18\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00%\\x00\\x0...\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n63.64\n38.43\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n3230087\n4200927\n18\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0f\\x00...\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n32.11\n28.26\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n3230088\n4200928\n18\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x0...\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n142.41\n59.90\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n3230089\n4200929\n18\nb'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1f\\x00...\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n72.51\n45.80\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n\n\n3230090 rows × 13 columns\n\n\n\n\nprint(\"\\nProcessing Summary:\")\nprint(f\"Expected files: {stats['expected_files']}\")\nprint(f\"Successfully processed: {stats['successful_reads']}\")\nprint(f\"Failed reads: {stats['failed_reads']}\")\nprint(f\"Total features: {stats['total_features']}\")\n\nprint(\"\\nUpload Date Range:\")\nprint(f\"Earliest: {stats['upload_date_range']['earliest']}\")\nprint(f\"Latest: {stats['upload_date_range']['latest']}\")\n\nprint(\"\\nDataFrame Info:\")\nprint(df.info())\n\n\nProcessing Summary:\nExpected files: 401\nSuccessfully processed: 383\nFailed reads: 18\nTotal features: 3230090\n\nUpload Date Range:\nEarliest: 2024-03-15 19:22:56\nLatest: 2024-10-21 14:26:05\n\nDataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3230090 entries, 0 to 3230089\nData columns (total 13 columns):\n #   Column                Dtype         \n---  ------                -----         \n 0   buil_fid              int64         \n 1   subproj_id            string        \n 2   geometry              object        \n 3   source_file           object        \n 4   province_code         object        \n 5   processing_timestamp  datetime64[us]\n 6   original_crs          object        \n 7   area                  float64       \n 8   perimeter             float64       \n 9   upload_timestamp      datetime64[ns]\n 10  quality_comments      object        \n 11  geometry_crs          object        \n 12  has_valid_geometry    bool          \ndtypes: bool(1), datetime64[ns](1), datetime64[us](1), float64(2), int64(1), object(6), string(1)\nmemory usage: 298.8+ MB\nNone\n\n\n\npl_df = to_polars(df)\n\n\nPolars DataFrame Info:\nRows: 3,230,090\nColumns: 13\n\nMemory usage by column:\n8215651270\n\nGeometry column type: Binary\n\n\n\npl_df\n\n\nshape: (3_230_090, 13)\n\n\n\nbuil_fid\nsubproj_id\ngeometry\nsource_file\nprovince_code\nprocessing_timestamp\noriginal_crs\narea\nperimeter\nupload_timestamp\nquality_comments\ngeometry_crs\nhas_valid_geometry\n\n\ni64\ni64\nbinary\ncat\ncat\ndatetime[μs]\nstr\nf64\nf64\ndatetime[μs]\nstr\nstr\nbool\n\n\n\n\n5890242\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\xa65\\x0a\\x0evAO\\xc0[\\x1e|&\\xdc(G@\\xadM!\\x0dvAO\\xc0hR\\xb2\\x8c\\xdc(G@-\\xceM&gt;vAO\\xc0\\xab\\x1c\\xe8\\x8c\\xdc(G\"…\n\"PE_Stracthcona_GE01_20221103.z…\n\"PE\"\n2025-01-13 11:08:24.975860\n\"EPSG:4617\"\nnull\nnull\n2024-10-21 14:26:05\n\"NaN in file for area; NaN in f…\n\"EPSG:4326\"\ntrue\n\n\n5890243\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00#\\x00\\x00\\x00\\xb36\\x04\\x7f\\xc7@O\\xc0\\xf4\\x9f\\xb15\\xd6(G@E@g~\\xc7@O\\xc09\\xbf\\xd5y\\xd6(G@N\\xb3\\x93\\xaf\\xc7@O\\xc0\\x98\\x1f\\x0cz\\xd6(G\"…\n\"PE_Stracthcona_GE01_20221103.z…\n\"PE\"\n2025-01-13 11:08:24.975860\n\"EPSG:4617\"\nnull\nnull\n2024-10-21 14:26:05\n\"NaN in file for area; NaN in f…\n\"EPSG:4326\"\ntrue\n\n\n5890244\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\xe1\\x91.\\xb6\\xb8@O\\xc0of\\xfaz\\xd5(G@}@W\\xb4\\xb8@O\\xc0f\\xc3fG\\xd6(G@\\xfa\\xb2\\x83\\xe5\\xb8@O\\xc0{0\\x9dG\\xd6(G\"…\n\"PE_Stracthcona_GE01_20221103.z…\n\"PE\"\n2025-01-13 11:08:24.975860\n\"EPSG:4617\"\nnull\nnull\n2024-10-21 14:26:05\n\"NaN in file for area; NaN in f…\n\"EPSG:4326\"\ntrue\n\n\n5890245\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\xd4'\\xaa\\xab\\xd2@O\\xc0%\\xaa\\x08\\x88\\xd2(G@\\xberp\\xaa\\xd2@O\\xc0S\\xe9P\\x10\\xd3(G@\\xd5\\xdf\\x9c\\xdb\\xd2@O\\xc0\\x0f@\\x87\\x10\\xd3(G\"…\n\"PE_Stracthcona_GE01_20221103.z…\n\"PE\"\n2025-01-13 11:08:24.975860\n\"EPSG:4617\"\nnull\nnull\n2024-10-21 14:26:05\n\"NaN in file for area; NaN in f…\n\"EPSG:4326\"\ntrue\n\n\n5890285\n497\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00)\\x00\\x00\\x00\\xbc\\xe7\\x1d`qAO\\xc08e\\xd0\\xc9O)G@\\xeb\\x91\\x82_qAO\\xc0\\x10\\x85\\xf4\\x0dP)G@O\\xe1\\xaf\\x90qAO\\xc0,T*\\x0eP)G\"…\n\"PE_Stracthcona_GE01_20221103.z…\n\"PE\"\n2025-01-13 11:08:24.975860\n\"EPSG:4617\"\nnull\nnull\n2024-10-21 14:26:05\n\"NaN in file for area; NaN in f…\n\"EPSG:4326\"\ntrue\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n4200925\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\xf0'\\x8b\\x08\\x9d\\x83`\\xc0a\\xaa\\xbe\\x0a\\x9c\\x02K@\\xbaR\\x81\\xfa\\x9c\\x83`\\xc0~2e\\x09\\x9c\\x02K@\\xdb\\xa7\\xee\\xf9\\x9c\\x83`\\xc0\\xd3Ov*\\x9c\\x02K\"…\n\"BC_Masset_GE01_20130828.zip\"\n\"BC\"\n2025-01-13 11:20:46.175574\n\"EPSG:4617\"\n53.61\n35.21\n2024-03-15 19:22:56\n\"\"\n\"EPSG:4326\"\ntrue\n\n\n4200926\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00%\\x00\\x00\\x00\\xc5\\xdc(Zw\\x83`\\xc0\\xd3\\xc7!\\x0a\\x9c\\x02K@^\\x99\\xa9uw\\x83`\\xc0\\x1f\\xe5\\xe6-\\x9c\\x02K@\\x95\\xcc\\x83tw\\x83`\\xc0I\\x0e\\x09p\\x9c\\x02K\"…\n\"BC_Masset_GE01_20130828.zip\"\n\"BC\"\n2025-01-13 11:20:46.175574\n\"EPSG:4617\"\n63.64\n38.43\n2024-03-15 19:22:56\n\"\"\n\"EPSG:4326\"\ntrue\n\n\n4200927\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\xf5\\x18x\\x8ex\\x83`\\xc0\\xee\\x10\\xf6\\xf1\\x95\\x02K@vKn\\x80x\\x83`\\xc0\\xf5\\x10\\x9c\\xf0\\x95\\x02K@}\\xbe\\x06\\x8bx\\x83`\\xc0[\\x8e\\\\xb8\\x96\\x02K\"…\n\"BC_Masset_GE01_20130828.zip\"\n\"BC\"\n2025-01-13 11:20:46.175574\n\"EPSG:4617\"\n32.11\n28.26\n2024-03-15 19:22:56\n\"\"\n\"EPSG:4326\"\ntrue\n\n\n4200928\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00'\\x96\\x14\\x8d\\xa5\\x83`\\xc0Q\\x9bW\\xd3\\x9b\\x02K@\\x10l\\x1e\\x9b\\xa5\\x83`\\xc0(\\xf3\\xb0\\xd4\\x9b\\x02K@\\x15ZA\\x98\\xa5\\x83`\\xc0\\xdc\\x8f\\x06z\\x9c\\x02K\"…\n\"BC_Masset_GE01_20130828.zip\"\n\"BC\"\n2025-01-13 11:20:46.175574\n\"EPSG:4617\"\n142.41\n59.9\n2024-03-15 19:22:56\n\"\"\n\"EPSG:4326\"\ntrue\n\n\n4200929\n18\nb\"\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1f\\x00\\x00\\x000\\xaa\\xbf~\\xa5\\x83`\\xc0\\x07\\xfb\\x09\\xb7\\x98\\x02K@\\xa8%\\xdd\\xa8\\xa5\\x83`\\xc0K\\x02\\x16\\xbb\\x98\\x02K@\\x9c\\xc2o\\xa9\\xa5\\x83`\\xc0\\xe4\\xe2\\x04\\x9a\\x98\\x02K\"…\n\"BC_Masset_GE01_20130828.zip\"\n\"BC\"\n2025-01-13 11:20:46.175574\n\"EPSG:4617\"\n72.51\n45.8\n2024-03-15 19:22:56\n\"\"\n\"EPSG:4326\"\ntrue\n\n\n\n\n\n\n\n\nStats with Polars\nMore to come with geom\n\n# Basic province-level stats\nbasic_stats = pl_df.group_by('province_code').agg([\n    pl.len().alias('building_count'),\n    pl.col('area').mean().alias('mean_area'),\n    pl.col('area').std().alias('std_area'),\n    pl.col('perimeter').mean().alias('mean_perimeter'),\n    pl.col('area').sum().alias('total_area')\n]).sort('province_code')\n\n\nbasic_stats\n\n\nshape: (12, 6)\n\n\n\nprovince_code\nbuilding_count\nmean_area\nstd_area\nmean_perimeter\ntotal_area\n\n\ncat\nu32\nf64\nf64\nf64\nf64\n\n\n\n\n\"PE\"\n192296\nnull\nnull\nnull\n0.0\n\n\n\"PEIGeorgetown.zip\"\n9816\nnull\nnull\nnull\n0.0\n\n\n\"QC\"\n901457\n223.736861\n770.736915\n65.636496\n2.0169e8\n\n\n\"YT1.zip\"\n244\n135.238443\n249.334182\n47.854795\n32998.18\n\n\n\"ON\"\n969723\n328.974588\n1051.517039\n86.030279\n3.1901e8\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"ON3.zip\"\n5120\n345.530316\n1107.940492\n82.701141\n1.7691e6\n\n\n\"NL\"\n92969\n198.123731\n508.20816\n63.938278\n1.8419e7\n\n\n\"NS7.zip\"\n632\n102.812785\n127.462484\n43.903608\n64977.68\n\n\n\"NB\"\n924115\n146.240881\n479.84233\n51.185004\n1.3514e8\n\n\n\"BC\"\n8216\n223.449869\n499.705411\n67.528354\n1.8359e6\n\n\n\n\n\n\n\n# Detailed province-level stats\ndetailed_stats = pl_df.group_by('province_code').agg([\n    pl.len().alias('building_count'),\n    pl.col('area').mean().alias('mean_area'),\n    pl.col('area').median().alias('median_area'),\n    pl.col('area').min().alias('min_area'),\n    pl.col('area').max().alias('max_area'),\n    pl.col('area').quantile(0.95).alias('p95_area'),\n    pl.col('perimeter').mean().alias('mean_perimeter'),\n    pl.col('subproj_id').n_unique().alias('num_subprojects'),\n    pl.col('upload_timestamp').max().alias('latest_upload')\n])\n\n\ndetailed_stats\n\n\nshape: (12, 10)\n\n\n\nprovince_code\nbuilding_count\nmean_area\nmedian_area\nmin_area\nmax_area\np95_area\nmean_perimeter\nnum_subprojects\nlatest_upload\n\n\ncat\nu32\nf64\nf64\nf64\nf64\nf64\nf64\nu32\ndatetime[μs]\n\n\n\n\n\"PE\"\n192296\nnull\nnull\nnull\nnull\nnull\nnull\n26\n2024-10-21 14:26:05\n\n\n\"PEIGeorgetown.zip\"\n9816\nnull\nnull\nnull\nnull\nnull\nnull\n1\n2024-10-21 14:24:40\n\n\n\"QC\"\n901457\n223.736861\n152.89\n0.0\n120224.49\n466.35\n65.636496\n43\n2024-03-15 19:30:31\n\n\n\"YT1.zip\"\n244\n135.238443\n110.19\n5.5\n3090.25\n268.62\n47.854795\n1\n2024-03-15 19:30:29\n\n\n\"ON\"\n969723\n328.974588\n197.0\n0.0\n161079.91\n884.25\n86.030279\n35\n2024-03-15 19:29:29\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ON3.zip\"\n5120\n345.530316\n192.25\n5.25\n36593.5\n853.75\n82.701141\n1\n2024-03-15 19:28:10\n\n\n\"NL\"\n92969\n198.123731\n149.25\n1.0\n57497.62\n438.25\n63.938278\n15\n2024-03-15 19:28:03\n\n\n\"NS7.zip\"\n632\n102.812785\n90.56\n5.72\n2830.37\n217.29\n43.903608\n1\n2024-03-15 19:27:59\n\n\n\"NB\"\n924115\n146.240881\n105.12\n0.0\n76104.36\n309.26\n51.185004\n238\n2024-03-15 19:27:49\n\n\n\"BC\"\n8216\n223.449869\n163.07\n5.4\n20237.75\n538.75\n67.528354\n13\n2024-03-15 19:23:08\n\n\n\n\n\n\n\n# Time-based stat\ntemporal_stats = pl_df.group_by([\n    pl.col('upload_timestamp').dt.year().alias('year'),\n    'province_code'\n]).agg([\n    pl.len().alias('buildings_added'),\n    pl.col('area').sum().alias('total_area_added')\n])\n\n\ntemporal_stats\n\n\nshape: (12, 4)\n\n\n\nyear\nprovince_code\nbuildings_added\ntotal_area_added\n\n\ni32\ncat\nu32\nf64\n\n\n\n\n2024\n\"PEIGeorgetown.zip\"\n9816\n0.0\n\n\n2024\n\"NB\"\n924115\n1.3514e8\n\n\n2024\n\"QC\"\n901457\n2.0169e8\n\n\n2024\n\"NS\"\n125356\n2.1832e7\n\n\n2024\n\"ON3.zip\"\n5120\n1.7691e6\n\n\n…\n…\n…\n…\n\n\n2024\n\"NS7.zip\"\n632\n64977.68\n\n\n2024\n\"ON\"\n969723\n3.1901e8\n\n\n2024\n\"PE\"\n192296\n0.0\n\n\n2024\n\"ON8.zip\"\n146\n25721.16\n\n\n2024\n\"BC\"\n8216\n1.8359e6\n\n\n\n\n\n\n\n# Province and source file stats\nhierarchical_stats = pl_df.group_by([\n    'province_code',\n    'source_file'\n]).agg([\n    pl.len().alias('building_count'),\n    pl.col('area').mean().alias('mean_area')\n])\n\n\nhierarchical_stats\n\n\nshape: (383, 4)\n\n\n\nprovince_code\nsource_file\nbuilding_count\nmean_area\n\n\ncat\ncat\nu32\nf64\n\n\n\n\n\"NB\"\n\"NB_DNR_2017_r.zip\"\n4664\n115.892071\n\n\n\"NB\"\n\"NB_DNR_2014_c.zip\"\n2527\n122.496347\n\n\n\"NL\"\n\"NL_BigPond_WV02_20210818_A.zip\"\n199\n78.414573\n\n\n\"NB\"\n\"NB_DNR_2017_t.zip\"\n724\n78.942238\n\n\n\"QC\"\n\"QC_LacBerthet_WV02_20190624_A.…\n1204\n189.629061\n\n\n…\n…\n…\n…\n\n\n\"ON\"\n\"ON_Peawanuck_WV03_20210707.zip\"\n5\n12.41\n\n\n\"NB\"\n\"NB_Edmundston_2016.zip\"\n13437\n163.377924\n\n\n\"NB\"\n\"NB_SNB_2022_f.zip\"\n2057\n98.838352\n\n\n\"NB\"\n\"NB_DNR_2020_o.zip\"\n7106\n130.566295\n\n\n\"ON\"\n\"ON_Bradford_WV03_20190804.zip\"\n37737\n198.00847\n\n\n\n\n\n\n\n\nGet rid of overlapping building footprints\n\n# Basic analysis\nresults = analyze_overlaps(df)\nprint(\"\\nOverlap Statistics:\")\nfor k, v in results['overlap_stats'].items():\n    print(f\"{k}: {v}\")\n\n\nOverlap Statistics:\ntotal_buildings: 3230090\nbuildings_with_overlaps: 1061279\noverlap_pairs: 1533274\n\n\n\n#results = None\nresults = analyze_overlaps(df, merge=True)\n\n\nfinal_gdf = results['merged_footprints']\n\n\nfinal_gdf\n#final_gdf = final_gdf.drop(columns=['merged_count'])\n\n\n\n\n\n\n\n\ngeometry\narea\nperimeter\nsubproj_id\nbuil_fid\nsource_file\nprovince_code\nprocessing_timestamp\noriginal_crs\nupload_timestamp\nquality_comments\ngeometry_crs\nhas_valid_geometry\n\n\n\n\n0\nPOLYGON ((-62.51144 46.31923, -62.51145 46.319...\n70.940533\n42.849899\n&lt;NA&gt;\n-2\nNaN\nNaN\nNaT\nNaN\nNaT\nNaN\nNaN\nNaN\n\n\n1\nPOLYGON ((-62.50609 46.31904, -62.5061 46.3190...\n353.344646\n86.594767\n&lt;NA&gt;\n-2\nNaN\nNaN\nNaT\nNaN\nNaT\nNaN\nNaN\nNaN\n\n\n2\nPOLYGON ((-62.50565 46.31902, -62.50565 46.319...\n122.221836\n50.871901\n&lt;NA&gt;\n-2\nNaN\nNaN\nNaT\nNaN\nNaT\nNaN\nNaN\nNaN\n\n\n3\nPOLYGON ((-62.50643 46.31894, -62.50644 46.318...\n136.113291\n53.382713\n&lt;NA&gt;\n-2\nNaN\nNaN\nNaT\nNaN\nNaT\nNaN\nNaN\nNaN\n\n\n4\nPOLYGON ((-62.47598 46.27649, -62.47598 46.276...\n165.795783\n65.985901\n&lt;NA&gt;\n-2\nNaN\nNaN\nNaT\nNaN\nNaT\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2725989\nPOLYGON ((-131.96859 54.03145, -131.9686 54.03...\n80.510000\n48.370000\n18\n4200822\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n2725990\nPOLYGON ((-132.17469 54.03467, -132.1747 54.03...\n86.490000\n51.780000\n18\n4200826\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n2725991\nPOLYGON ((-132.17876 54.03433, -132.17878 54.0...\n40.210000\n29.400000\n18\n4200828\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n2725992\nPOLYGON ((-132.01842 54.01891, -132.01848 54.0...\n137.780000\n74.320000\n18\n4200910\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n2725993\nPOLYGON ((-132.10694 54.02104, -132.10695 54.0...\n12.820000\n17.260000\n18\n4200919\nBC_Masset_GE01_20130828.zip\nBC\n2025-01-14 16:09:01.269525\nEPSG:4617\n2024-03-15 19:22:56\n\nEPSG:4326\nTrue\n\n\n\n\n2725994 rows × 13 columns\n\n\n\n\n\nCompare schemas\n\nfile_paths = [\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/BC_CapeBall_WV03_20210702.zip',\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_Bouctouche_2016.zip',\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/QC_RockForest_WV03_20220930.zip'\n]\n\nfile_paths = [\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/BC_Masset_WV02_20160607.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/BC_PortClements_GE01_20180930.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_AnseBleue_WV03_20200807.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_Brantville_WV02_20210825.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_DNR_2013_a.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_DNR_2015_a.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_DNR_2017_s.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_DNR_2019_an.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_Fairisle_WV02_20210919_A.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_SNB_2022_g.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/ON_RideauFerry_GE01_20180730.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/ON_TalbotRiver_WV02_20180710.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/PEIGeorgetown.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/QC_Boisbriand_WV02_20210524.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/QC_ZecTawachiche_WV02_20190825_B.zip\",\n\"https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/YT1.zip\"]\n\n\ncomparison = compare_gpkg_schemas(file_paths)\n\n\nprint_schema_comparison(comparison)\n\n\ntest_files = [\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/BC_CapeBall_WV03_20210702.zip',\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/NB_Bouctouche_2016.zip',\n    'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/QC_RockForest_WV03_20220930.zip'\n]\n\n\ncombined_gdf, stats = read_and_concat_gpkgs(test_files)\n\n\n# Print processing statistics\nprint(\"\\nProcessing Statistics:\")\nprint(f\"Total files processed: {stats['total_files']}\")\nprint(f\"Successful reads: {stats['successful_reads']}\")\nprint(f\"Failed reads: {stats['failed_reads']}\")\nprint(f\"Total features: {stats['total_features']}\")\n\nif stats['crs_transforms']:\n    print(\"\\nCRS Transformations:\")\n    for transform in stats['crs_transforms']:\n        print(f\"File: {transform['file']}\")\n        print(f\"  From: {transform['from']}\")\n        print(f\"  To: {transform['to']}\")\n\nif stats['errors']:\n    print(\"\\nErrors encountered:\")\n    for error in stats['errors']:\n        print(f\"File: {error['file']}\")\n        print(f\"  Error: {error['error']}\")\n\n# Display information about the combined GeoDataFrame\nprint(\"\\nCombined GeoDataFrame Info:\")\nprint(combined_gdf.info())\n\n\n\nWrite to file\n\n# Temporarily write to parquet\n#table_path = save_processed_data(df, partition_by=\"province_code\", geometry_encoding=\"geoarrow\")\n#table_path = save_processed_data(df, geometry_encoding=\"geoarrow\")\ntable_path = save_processed_data(final_gdf, geometry_encoding=\"geoarrow\")\n\nSaved data to: c:\\Users\\ymoisan\\github\\nbcmc\\nbs\\processed_data\\geoai_buildings_20250115_1352_geoarrow.parquet\n\n\n\nzip_size = stats['total_size_mb']\nprint(f\"Original ZIP files size: {zip_size:.1f} MB\")\n\n\ndf_from_parquet = load_processed_data()\n\n\ndf_from_parquet\n\n\n\nFTP tests\n\nfrom ftplib import FTP\nfrom datetime import datetime\n\ntry:\n    # Connect to FTP server\n    print(\"Connecting to FTP server...\")\n    ftp = FTP('ftp.maps.canada.ca')\n    ftp.login()  # anonymous login\n    \n    print(\"\\nNavigating to GPKG directory...\")\n    ftp.cwd('/pub/nrcan_rncan/vector/geobase_geoai_geoia/GPKG/')\n    \n    # Get file list with details\n    files = []\n    ftp.dir(lambda x: files.append(x))\n    \n    print(\"\\nGPKG ZIP files and their timestamps:\")\n    print(\"-\" * 70)\n    \n    zip_files = []\n    for file_info in files:\n        if '.zip' in file_info:\n            parts = file_info.split()\n            # Get file size\n            size = parts[4]\n            # Combine date parts\n            date_str = f\"{parts[5]} {parts[6]} {parts[7]}\"\n            filename = parts[-1]\n            print(f\"{filename:&lt;50} {date_str:&gt;15} {size:&gt;10} bytes\")\n            zip_files.append({\n                'filename': filename,\n                'upload_date': date_str,\n                'size': int(size)\n            })\n    \n    print(f\"\\nTotal ZIP files found: {len(zip_files)}\")\n    \nexcept Exception as e:\n    print(f\"Error accessing FTP: {str(e)}\")\nfinally:\n    try:\n        ftp.quit()\n    except:\n        pass\n\n\nimport lonboard\n\n\nlonboard.__version__\n\n'0.10.3'",
    "crumbs": [
      "GeoAI buildings extraction"
    ]
  },
  {
    "objectID": "cmcade-ingest.html",
    "href": "cmcade-ingest.html",
    "title": "CMC ADE – ingest from tar files",
    "section": "",
    "text": "import datetime as dt\nimport os, getpass\nimport pathlib\nimport tarfile\nimport time\n\nimport daft\nimport deltalake\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom deltalake import DeltaTable  # S3FileSystem ??\nfrom deltalake.writer import write_deltalake\nfrom great_tables import GT, html, md\nfrom great_tables.data import islands\nfrom tabulate import tabulate\nfrom tqdm.notebook import tqdm\nfrom my_nb_utils import (\n    extract_xml_data_to_pd,\n    get_latest_tag_and_date,\n    memused,\n    print_system_usage,\n)\nmodules = {\n    \"pandas\": {\n        \"name\": pd.__name__,\n        \"version\": pd.__version__,\n        \"url\": \"https://github.com/pandas-dev/pandas\",\n    },\n    \"polars\": {\n        \"name\": pl.__name__,\n        \"version\": pl.__version__,\n        \"url\": \"https://github.com/pola-rs/polars\",\n    },\n    \"pyarrow\": {\n        \"name\": pa.__name__,\n        \"version\": pa.__version__,\n        \"url\": \"https://github.com/apache/arrow\",\n    },\n    \"daft\": {\n        \"name\": daft.__name__,\n        \"version\": daft.__version__,\n        \"url\": \"https://github.com/Eventual-Inc/Daft\",\n    },\n    \"delta-io Rust\": {\n        \"name\": deltalake.__name__,\n        \"version\": deltalake.__version__,\n        \"url\": \"https://github.com/delta-io/delta-rs\",\n    },\n}\n\n# for module_info in modules.values():\n#     print(f\"Current version of {module_info['name']} is {module_info['version']}\")\n#     print(f\"GitHub repository URL: {module_info['url']}\")\n#     print()\n\nfor module_info in modules.values():\n    print(f\"Module: {module_info['name']}\")\n    print(f\"Current version: {module_info['version']}\")\n    repo_url = module_info[\"url\"]\n    name, latest_version, dt = get_latest_tag_and_date(repo_url)\n    print(f\"Latest version: {latest_version}\")\nmemused()\nprint_system_usage()",
    "crumbs": [
      "CMC ADE -- ingest from tar files"
    ]
  },
  {
    "objectID": "cmcade-ingest.html#convert-the-pandas-dataframe-to-a-polars-dataframe",
    "href": "cmcade-ingest.html#convert-the-pandas-dataframe-to-a-polars-dataframe",
    "title": "CMC ADE – ingest from tar files",
    "section": "Convert the Pandas DataFrame to a Polars DataFrame",
    "text": "Convert the Pandas DataFrame to a Polars DataFrame\n\n# This is temporary.  tar files should be writable directly in a Polars df\n# We are then casting Polars data types to each column\n# BECAUSE THIS DOES NOT WORK PROPERLY IN PANDAS : values are not floats\n# That  defeats the purpose of data analysis !\n\ndf = pl.from_pandas(final_df)\n\n# Define the column names and their corresponding types; aka data schema\ncolumn_types = {\n    \"name\": pl.String,\n    \"value\": pl.Float64,  # **Force casting to float did not work in Pandas !**\n    \"uom\": pl.String,\n    \"date_tm\": pl.Datetime,\n    \"stn_nam\": pl.String,\n    \"tc_id\": pl.String,\n    \"wmo_synop_id\": pl.String,\n    \"stn_elev\": pl.Float64,\n    \"data_pvdr\": pl.String,\n    \"msc_id\": pl.String,\n    \"clim_id\": pl.String,\n    \"lat\": pl.Float64,\n    \"long\": pl.Float64,\n}\n\nfor col, dtype in column_types.items():\n    # Check if the column exists in the DataFrame and if it's not already of the specified type\n    if col in df.columns and df[col].dtype != dtype:\n        df = df.select(\n            [\n                pl.when(pl.col(col).is_not_null(), pl.col(col))\n                .otherwise(pl.lit(None))\n                .alias(col)\n                if dtype == pl.String\n                else pl.col(col).cast(dtype).alias(col)\n            ]\n        )\n\n\ndf",
    "crumbs": [
      "CMC ADE -- ingest from tar files"
    ]
  },
  {
    "objectID": "cmcade-ingest.html#what-are-delta-lake-tables-and-why-use-them-over-individual-parquet-files",
    "href": "cmcade-ingest.html#what-are-delta-lake-tables-and-why-use-them-over-individual-parquet-files",
    "title": "CMC ADE – ingest from tar files",
    "section": "What are Delta Lake tables and why use them over individual Parquet files",
    "text": "What are Delta Lake tables and why use them over individual Parquet files\nDelta tables consist of metadata in a transaction log and data stored in Parquet files. Polars [or any dataframe library] can skip Parquet files based on metadata, but it needs to open up each file and read the metadata, which is slower that grabbing the file-level metadata directly from the transaction log.",
    "crumbs": [
      "CMC ADE -- ingest from tar files"
    ]
  },
  {
    "objectID": "cmcade-ingest.html#java-vs-rustpython",
    "href": "cmcade-ingest.html#java-vs-rustpython",
    "title": "CMC ADE – ingest from tar files",
    "section": "Java vs Rust/Python",
    "text": "Java vs Rust/Python\n“Rust deltalake” refers to the Rust API of delta-rs (no Spark dependency); this is what this notebook uses  “Python deltalake” refers to the Python API of delta-rs (no Spark dependency)",
    "crumbs": [
      "CMC ADE -- ingest from tar files"
    ]
  },
  {
    "objectID": "cmcade-ingest.html#ordering-and-partitioning",
    "href": "cmcade-ingest.html#ordering-and-partitioning",
    "title": "CMC ADE – ingest from tar files",
    "section": "Ordering and partitioning",
    "text": "Ordering and partitioning\nFrom the Delta Lake best practices page\n… optimizing the performance of your Delta tables … depends on your data ingestion into the Delta table and query patterns. You must understand your data and how users run queries to best leverage Delta Lake.\nThe idea is to colocate similar data in the same files to make file skipping more effective\nTwo approaches : Z ordering Hive-style partitioning\nYou can use Hive-style partitioning in conjunction with Z Ordering. You can partition a table by one column and Z Order by another. They’re different tactics that aim to help you skip more files and run queries faster.\nSee also tips on append-only tables, like our observations or any sensor measurement.",
    "crumbs": [
      "CMC ADE -- ingest from tar files"
    ]
  },
  {
    "objectID": "test_usd.html",
    "href": "test_usd.html",
    "title": "Tests",
    "section": "",
    "text": "from pxr import Usd, UsdGeom, UsdUtils, UsdShade, Sdf, Gf\nimport struct\nfrom pathlib import Path\nimport script\nfrom script import extract_gltf_from_b3dm, \\\n                   check_converter_availability, \\\n                   create_layer_from_geotiff, \\\n                   create_layer_from_3dtiles, \\\n                   create_stage, \\\n                   add_layer_to_stage, \\\n                   save_stage, \\\n                   create_layer_with_normals\nimport rasterio\nimport getpass\n\n#import importlib\n#importlib.reload(script)\n\n\n# This is needed to see something in usdview as values in lat/lon coordinates\n# show up clustered and are almost invisible.  Omniverse shold allow using no SCALE_FACTOR ?\nSCALE_FACTOR = 100000.0\n\n\nstage = Usd.Stage.CreateNew('HelloWorld.usda')\nxformPrim = UsdGeom.Xform.Define(stage, '/hello')\nspherePrim = UsdGeom.Sphere.Define(stage, '/hello/world')\nstage.GetRootLayer().Save()\n\n\nExtract glTF from 3DTiles/b3dm\n\nfull_path = getpass.getpass(\"Enter path to b3dm file: \")\nb3dm_file = Path(full_path) # Autobuilding lidar\n\n\nprint(f\"Extracting glTF data from {b3dm_file} to its corresponding .glb file\")\nextract_gltf_from_b3dm(b3dm_file)\n\n\ngltf_file = b3dm_file.with_suffix('.glb')\n\n\n# Create a new USD stage from the GLB file\ngltf_path = str(gltf_file)  # Convert Path to string\noutput_usd = gltf_file.with_suffix('.usda')\n\n\n\nConvert a small DEM to USD\n\ndem_file = Path(\"02EJ_5m_Cilp_4326.tif\")\n\n\nusd_output_file = dem_file.with_suffix('.usdc')\n# usd_filename = \"usd_output_file.usda\"\n# usd_output_file = Path(usd_filename)\n\n\n# 1. Create a new stage\nstage = create_stage(usd_output_file)\n\n\n# 2. Create the DEM layer from GeoTIFF\ndem_layer = create_layer_from_geotiff(dem_file, \n           sample_factor=1, \n           scale_factor=SCALE_FACTOR, \n           material_type='terrain',\n           bbox=(-63.679503,44.564071,-63.565863,44.625435)\n           )\n\n\n\n\n\n\n\n\n\n\n\n# 3. Add the DEM layer to the stage\nadd_layer_to_stage(stage, dem_layer)\n\n\nMesh Statistics:\nNumber of points: 3351740\nNumber of faces: 6695808\nZ range: -0.9736483693122864 to 133.67996215820312\n\n\n\n\n3DTiles Buildings in USD\n\nblg_path = Path(\"C:/Users/ymoisan/3dtiles/Cesium-1.123/Halifax_0_0_1_aoi_3D_4326/tileset.json\")\n\n\nbldg_layer = create_layer_from_3dtiles(blg_path, scale_factor=1.0, z_offset=50.0)\n\n\nadd_layer_to_stage(\n    stage,\n    bldg_layer,\n    \"/World/Buildings\",  # Different path in the USD hierarchy\n)\n\n\n\nGPKG Buildings in USD\n\n# Create and add buildings layer from GeoPackage\nbuildings_gpkg_layer = create_layer_with_normals(\"halifax-aoi-plus_4326.gpkg\", material_type='building', scale_factor=SCALE_FACTOR, z_offset=10.0)\n\n\n# Create and add buildings layer from GeoPackage\nadd_layer_to_stage(stage, buildings_gpkg_layer, \"/World/Buildings\")\n\n\nMesh Statistics:\nNumber of points: 1447700\nNumber of faces: 983390\nZ range: 30.219999313354492 to 137.86000061035156\n\n\n\n\nSave stage\n\n# 6. Save the stage once after all layers are added\nsave_stage(stage)\n\nWriting USD file... |\n\n\n\n\nOdd stuff\n\ndef convert_usdc_to_usdz(usdc_path: str, usdz_path: str):\n    # Open the existing USDC stage\n    stage = Usd.Stage.Open(usdc_path)\n    if not stage:\n        raise RuntimeError(f\"Could not open stage at {usdc_path}\")\n    \n    # Package the stage into a USDZ file.\n    # (This requires a USD build with USDZ support)\n    UsdUtils.CreateNewUsdzPackage(stage, usdz_path)\n    print(f\"Converted {usdc_path} to {usdz_path}\")",
    "crumbs": [
      "Tests"
    ]
  }
]