{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf79ce1d-5d4a-4d50-9fb2-4cb8b33ccaa1",
   "metadata": {},
   "source": [
    "# CMC ADE -- ingest from tar files\n",
    "`FR`\n",
    "Le présent notebook permet d'extraire les données d'observations en format [swob-ml](https://dd.weather.gc.ca/observations/doc/Met-ML-SchemaDescriptionV2_f.pdf) qui sont concaténées dans un fichier [tar](https://fr.wikipedia.org/wiki/Tar_(informatique)) [^1]. Les observations sont ajoutées dans un dataframe et sont réécrites sur disque en format [Delta Lake](https://delta-io.github.io/delta-rs/).\n",
    "\n",
    "&#9658; Les fichiers (ou \"tables\") Delta Lake que vous trouverez plus bas contiennent des données sur plusieurs jours en mars 2024.\n",
    "\n",
    "`EN`\n",
    "This notebook allows extacting observations encoded in [swob-ml](https://dd.weather.gc.ca/observations/doc/Met-ML-SchemaDescriptionV2_e.pdf)  files concatenated in [tar files](https://en.wikipedia.org/wiki/Tar_(computing))[^2].  As they are extracted from the swob-ml, observations are stored in a dataframe and later written to disk as [Delta Lake](https://delta-io.github.io/delta-rs/) tables.\n",
    "\n",
    "&#9658; Delta Lake tables found below contain obserrvation data from several days in March 2024.\n",
    "\n",
    "[^1]: Voir le répertoire `/space/hall6/sitestore/eccc/prod/hubs/ade/rawdata/swob/ca/` pour des exemples.\n",
    "[^2]: See the `/space/hall6/sitestore/eccc/prod/hubs/ade/rawdata/swob/ca/` directory for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6a607-5fd3-4399-b471-0af8ec9a4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os, getpass\n",
    "import pathlib\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import daft\n",
    "import deltalake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from deltalake import DeltaTable  # S3FileSystem ??\n",
    "from deltalake.writer import write_deltalake\n",
    "from great_tables import GT, html, md\n",
    "from great_tables.data import islands\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98345ef9-7618-4215-a00d-d4c3dea4ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nb_utils import (\n",
    "    extract_xml_data_to_pd,\n",
    "    get_latest_tag_and_date,\n",
    "    memused,\n",
    "    print_system_usage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bc22a-e151-4973-ba8c-2872051b1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = {\n",
    "    \"pandas\": {\n",
    "        \"name\": pd.__name__,\n",
    "        \"version\": pd.__version__,\n",
    "        \"url\": \"https://github.com/pandas-dev/pandas\",\n",
    "    },\n",
    "    \"polars\": {\n",
    "        \"name\": pl.__name__,\n",
    "        \"version\": pl.__version__,\n",
    "        \"url\": \"https://github.com/pola-rs/polars\",\n",
    "    },\n",
    "    \"pyarrow\": {\n",
    "        \"name\": pa.__name__,\n",
    "        \"version\": pa.__version__,\n",
    "        \"url\": \"https://github.com/apache/arrow\",\n",
    "    },\n",
    "    \"daft\": {\n",
    "        \"name\": daft.__name__,\n",
    "        \"version\": daft.__version__,\n",
    "        \"url\": \"https://github.com/Eventual-Inc/Daft\",\n",
    "    },\n",
    "    \"delta-io Rust\": {\n",
    "        \"name\": deltalake.__name__,\n",
    "        \"version\": deltalake.__version__,\n",
    "        \"url\": \"https://github.com/delta-io/delta-rs\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# for module_info in modules.values():\n",
    "#     print(f\"Current version of {module_info['name']} is {module_info['version']}\")\n",
    "#     print(f\"GitHub repository URL: {module_info['url']}\")\n",
    "#     print()\n",
    "\n",
    "for module_info in modules.values():\n",
    "    print(f\"Module: {module_info['name']}\")\n",
    "    print(f\"Current version: {module_info['version']}\")\n",
    "    repo_url = module_info[\"url\"]\n",
    "    name, latest_version, dt = get_latest_tag_and_date(repo_url)\n",
    "    print(f\"Latest version: {latest_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ce817-9146-4b79-9876-ce085d4e7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "memused()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f46eb8-e6bd-44cc-9718-2e9c4ed068a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_system_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546bccb1-46c4-43cb-8355-d80597576b84",
   "metadata": {},
   "source": [
    "# Append data in a Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddb533-69a1-42ea-962b-207874d33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = getpass.getpass(prompt='Base directory for TAR files : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5495b-35ab-47a7-b4ba-361b5ae3934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"2024032718_tar\", \"2024032712_tar\", \"2024032706_tar\", \"2024032700_tar\"]\n",
    "filenames_18 = [\"2024032718_tar\"]\n",
    "filenames_12 = [\"2024032712_tar\"]\n",
    "filenames_06 = [\"2024032706_tar\"]\n",
    "filenames_00 = [\"2024032700_tar\"]\n",
    "tar_files = [base_dir + filename for filename in filenames]\n",
    "tar_files_18 = [base_dir + filename for filename in filenames_18]\n",
    "tar_files_12 = [base_dir + filename for filename in filenames_12]\n",
    "tar_files_06 = [base_dir + filename for filename in filenames_06]\n",
    "tar_files_00 = [base_dir + filename for filename in filenames_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1fcb7-2ef4-45eb-b7ec-3f2b2b847a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed682c28-2ab4-4887-ad94-ea4419ae58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list to store DataFrames for each XML file\n",
    "dfs = []\n",
    "total_bad_data_records = 0\n",
    "\n",
    "for file in tqdm(tar_files):\n",
    "    print(f\"Processing {file}\")\n",
    "    with tarfile.open(file, \"r\") as tar:\n",
    "        for member in tqdm(tar.getmembers()):\n",
    "            # Check if the member is a file\n",
    "            if member.isfile():\n",
    "                # Extract the file content\n",
    "                xml_data = tar.extractfile(member)\n",
    "                # Extract data from XML and append to list of DataFrames\n",
    "                df, bad_data_records = extract_xml_data_to_pd(xml_data)\n",
    "                total_bad_data_records += bad_data_records\n",
    "                dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(\n",
    "    f\"Total number of skipped records due to missing or invalid MEASUREMENT data = {total_bad_data_records}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb5329-4fbc-4461-ba66-5189014f0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   18         12         06         00\n",
    "# name            167092     196563     228209     196342\n",
    "# value           167092     196563     228209     196342\n",
    "final_df  # Pandas df; does not show column type; see Polars df below\n",
    "# final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9307b-a5b6-441c-821f-1204f73325bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives\n",
    "# Data columns (total 13 columns):\n",
    "# #   Column        Non-Null Count   Dtype\n",
    "# ---  ------        --------------   -----\n",
    "# 0   name          792644 non-null  object\n",
    "# 1   value         753137 non-null  object\n",
    "#\n",
    "# Where `value` is not a float !!\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b8b56-5927-4aa3-9eab-7d2f136c4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Number of unique \"station name\" values = {len(list(final_df['stn_nam'].unique()))}')\n",
    "# print(f'Number of unique  \"name\" values = {len(list(final_df['name'].unique()))}')\n",
    "# print(f'Number of unique \"wmo_synop_id\" values = {len(list(final_df['wmo_synop_id'].unique()))}')\n",
    "# print(f'Number of unique \"clim_id\" values = {len(list(final_df['clim_id'].unique()))}')\n",
    "# print(f'Number of unique \"msc_id\" values = {len(list(final_df['msc_id'].unique()))}')\n",
    "# print(f'Number of unique \"latitude\" values = {len(list(final_df['lat'].unique()))}')\n",
    "# print(f'Number of unique \"longitude\" values = {len(list(final_df['long'].unique()))}')\n",
    "print(list(final_df['name'].unique()))\n",
    "# print(list(final_df['date_tm'].unique()))\n",
    "# type(list(final_df['date_tm'].unique())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bb217-6e3e-4527-a682-d7b4c08d94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write parquet files\n",
    "# final_df.to_parquet('20240327.parquet', engine='pyarrow', compression='snappy')\n",
    "# final_df.to_parquet('pyarrow_gzip.parquet', engine='pyarrow', compression='gzip')\n",
    "# final_df.to_parquet('pyarrow_brotli.parquet', engine='pyarrow', compression='brotli')\n",
    "# final_df.to_parquet('pyarrow_lz4.parquet', engine='pyarrow', compression='lz4')\n",
    "# final_df.to_parquet('pyarrow_zstd.parquet', engine='pyarrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f882157-59f2-49a7-99ef-87b364855286",
   "metadata": {},
   "source": [
    "## Convert the Pandas DataFrame to a Polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9677212-7b73-4600-b381-aa994f112cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is temporary.  tar files should be writable directly in a Polars df\n",
    "# We are then casting Polars data types to each column\n",
    "# BECAUSE THIS DOES NOT WORK PROPERLY IN PANDAS : values are not floats\n",
    "# That  defeats the purpose of data analysis !\n",
    "\n",
    "df = pl.from_pandas(final_df)\n",
    "\n",
    "# Define the column names and their corresponding types; aka data schema\n",
    "column_types = {\n",
    "    \"name\": pl.String,\n",
    "    \"value\": pl.Float64,  # **Force casting to float did not work in Pandas !**\n",
    "    \"uom\": pl.String,\n",
    "    \"date_tm\": pl.Datetime,\n",
    "    \"stn_nam\": pl.String,\n",
    "    \"tc_id\": pl.String,\n",
    "    \"wmo_synop_id\": pl.String,\n",
    "    \"stn_elev\": pl.Float64,\n",
    "    \"data_pvdr\": pl.String,\n",
    "    \"msc_id\": pl.String,\n",
    "    \"clim_id\": pl.String,\n",
    "    \"lat\": pl.Float64,\n",
    "    \"long\": pl.Float64,\n",
    "}\n",
    "\n",
    "for col, dtype in column_types.items():\n",
    "    # Check if the column exists in the DataFrame and if it's not already of the specified type\n",
    "    if col in df.columns and df[col].dtype != dtype:\n",
    "        df = df.select(\n",
    "            [\n",
    "                pl.when(pl.col(col).is_not_null(), pl.col(col))\n",
    "                .otherwise(pl.lit(None))\n",
    "                .alias(col)\n",
    "                if dtype == pl.String\n",
    "                else pl.col(col).cast(dtype).alias(col)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15165e09-8a47-4e70-b328-26f78a8cb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3235e8-fdca-4679-8d6b-63b03fb5f9fb",
   "metadata": {},
   "source": [
    "# Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2f844-1cfc-4e29-8995-2035e00f5ef8",
   "metadata": {},
   "source": [
    "## What are Delta Lake tables and why use them over individual Parquet files\n",
    "\n",
    "Delta tables consist of metadata in a transaction log and data stored in Parquet files.  Polars [or any dataframe library] can skip Parquet files based on metadata, but it needs to open up each file and read the metadata, which is slower that grabbing the file-level metadata directly from the transaction log.\n",
    "\n",
    "## Java vs Rust/Python\n",
    "\n",
    "\"Rust deltalake\" refers to the Rust API of delta-rs (no Spark dependency); this is what this notebook uses <br>\n",
    "\"Python deltalake\" refers to the Python API of delta-rs (no Spark dependency)\n",
    "\n",
    "## Ordering and partitioning\n",
    "\n",
    "From the [Delta Lake best practices page](https://delta-io.github.io/delta-rs/delta-lake-best-practices/)\n",
    "\n",
    "... optimizing the performance of your Delta tables ... depends on your data ingestion into the Delta table and query patterns. You must understand your data and how users run queries to best leverage Delta Lake.\n",
    "\n",
    "The idea is to colocate similar data in the same files to make file skipping more effective\n",
    "\n",
    "Two approaches :\n",
    "Z ordering\n",
    "Hive-style partitioning\n",
    "\n",
    "You can use Hive-style partitioning in conjunction with Z Ordering. You can partition a table by one column and Z Order by another. They’re different tactics that aim to help you skip more files and run queries faster.\n",
    "\n",
    "See also tips on append-only tables, like our observations or any sensor measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cc99d-cb2f-463f-8cc7-e4af911c0d76",
   "metadata": {},
   "source": [
    "# Delta Lake tests\n",
    "Writing with and without partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e5248-17c2-48ef-b80a-165910fce5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No optimization i.e. neither partitioning nor Z ordering\n",
    "# df.write_delta(\"tar_swob_no_optimization\") First write\n",
    "df.write_delta(\"tar_swob_no_optimization\", mode=\"append\")  # append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f3630-87e3-42de-a37e-3430c7478953",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -Rlht \"tar_swob_no_optimization\" | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffd68c-d5cd-4f9d-9a35-2d44586bcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_swob_no_optimization_dt = DeltaTable(\"tar_swob_no_optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de74fa8-3767-433c-9752-908cf96270ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_swob_no_optimization_dt.history()\n",
    "tar_swob_no_optimization_dt.version()\n",
    "tar_swob_no_optimization_dt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3229106-d77b-4b04-8366-326480743302",
   "metadata": {},
   "source": [
    "# todo : Optimization tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d3044-9b10-4491-9b57-2de8ce84d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open \"tar_swob_no_optimization\" as a Polars df\n",
    "# Save it back as a Delta table partitioned by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df75b84-bb9d-4a7d-b5fe-faec8af21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_delta(\"tar_swob_no_optimization\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88324f81-183e-49c0-917d-87e10b337fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define partition columns\n",
    "partition_cols = [\"stn_nam\"]\n",
    "\n",
    "df.write_delta(\n",
    "    \"tar_swob_P_by_stn_name\",\n",
    "    mode=\"append\",\n",
    "    delta_write_options={\"partition_by\": partition_cols},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb70f8-58bf-4a30-a4be-b3f18f6a14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Idéal à long terme puisque les noms de stations vont peu varier, mais les dates oui\n",
    "# !ls -Rlht \"tar_swob_P_by_stn_name\" | wc\n",
    "# # !ls -Rlht \"tar_swob_P_by_stn_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43870b5a-00a7-4ccd-bde1-b03e395afa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define partition columns\n",
    "# partition_cols = [\"date_tm\"]\n",
    "# \n",
    "# # Define Z-order columns\n",
    "# # zorder_cols = [\"stn_nam\"]\n",
    "# df.write_delta(\n",
    "#     \"tar_swob_P_by_date\",\n",
    "#     mode=\"append\",\n",
    "#     delta_write_options={\"partition_by\": partition_cols},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb0e72-9924-4301-a0d7-a2212c9b3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -Rlht \"tar_swob_P_by_date\" | wc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_polars_daft_deltalake",
   "language": "python",
   "name": "pandas_polars_daft_deltalake"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
